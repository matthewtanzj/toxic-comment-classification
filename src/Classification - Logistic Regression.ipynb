{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Liblinear to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = [\"C:/Users/serena/Downloads/liblinear-2.20/python\"] ## do change this :) \n",
    "for path in paths:\n",
    "    if path not in sys.path:\n",
    "        sys.path.insert(0, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from liblinear import * \n",
    "from liblinearutil import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stating the path of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_bow_path = \"../../data/data/train/train_data_bow.csv\"\n",
    "\n",
    "test_data_bow_path = \"../../data/data/test/test_data_bow.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_manual_path = \"../../data/data/train/train_data_manual.csv\"\n",
    "\n",
    "test_data_manual_path = \"../../data/data/test/test_data_manual.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_func(y_label, y_proba):\n",
    "    return log_loss(y_label, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_vector(bow_array, manual_array):\n",
    "    \n",
    "    num_obs = len(bow_array)\n",
    "    max_bow_features = len(bow_array[0])\n",
    "    features_compiled = []\n",
    "    \n",
    "    for i in range(0, num_obs):\n",
    "        \n",
    "        features = {}\n",
    "        bow_obs = bow_array[i]\n",
    "        manual_obs = manual_array[i]\n",
    "        \n",
    "        for index,value in enumerate(bow_obs):   \n",
    "            if value != 0:\n",
    "                features[(index + 1)] = value\n",
    "        \n",
    "        for index,value in enumerate(manual_obs):\n",
    "            if value != 0:\n",
    "                features[(index + 1 + max_bow_features)] = value\n",
    "        \n",
    "        features_compiled.extend([features])\n",
    "    \n",
    "    return features_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(start, end, step):\n",
    "    val = []\n",
    "    current = start\n",
    "    val.extend([current])\n",
    "    while current < end:\n",
    "        add = current * step\n",
    "        val.extend([add])\n",
    "        current = add\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the training data into k blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateSplits(num, nr_folds):\n",
    "    \n",
    "    samples_split = []\n",
    "    \n",
    "    choices = [i for i in range(0,num)]\n",
    "    \n",
    "    num_samples = int(num/ nr_folds)\n",
    "    num_samples_last = num - (int(num/ nr_folds) * (nr_folds - 1))\n",
    "    \n",
    "    for i in range(0, nr_folds):\n",
    "        \n",
    "        if i == (nr_folds -1):\n",
    "            picks = random.sample(choices, num_samples_last)\n",
    "            samples_split.append(picks)\n",
    "            choices = [x for x in choices if x not in picks]\n",
    "        \n",
    "        else:\n",
    "            picks = random.sample(choices, num_samples)\n",
    "            samples_split.append(picks)\n",
    "            choices = [x for x in choices if x not in picks]\n",
    "\n",
    "    return samples_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class data: \n",
    "    \n",
    "    def __init__(self, path_bow, path_manual, type_):\n",
    "        self.path_bow = path_bow\n",
    "        self.path_manual = path_manual\n",
    "        \n",
    "        self.bow_df = pd.read_csv(path_bow)\n",
    "        self.manual_df = pd.read_csv(path_manual)\n",
    "        self.num_obs = self.bow_df.shape[0]\n",
    "        self.type_ = type_\n",
    "        \n",
    "        if type_ == \"train\":\n",
    "            \n",
    "            self.bow = np.array(self.bow_df.iloc[:, 1:])\n",
    "            self.manual = np.array(self.manual_df.iloc[:, 8:])\n",
    "            self.features = get_feature_vector(self.bow, self.manual)\n",
    "            \n",
    "            self.toxic = list(self.manual_df.loc[:, \"toxic\"])\n",
    "            self.severe_toxic = list(self.manual_df.loc[:, \"severe_toxic\"])\n",
    "            self.obscene = list(self.manual_df.loc[:, \"obscene\"])\n",
    "            self.threat = list(self.manual_df.loc[:, \"threat\"])\n",
    "            self.insult = list(self.manual_df.loc[:, \"insult\"])\n",
    "            self.identity_hate = list(self.manual_df.loc[:, \"identity_hate\"])\n",
    "        \n",
    "        elif type_ == \"test\":\n",
    "            \n",
    "            self.bow = np.array(self.bow_df.iloc[:, 1:])\n",
    "            self.manual = np.array(self.manual_df.iloc[:, 2:])\n",
    "            self.features = get_feature_vector(self.bow, self.manual)\n",
    "            \n",
    "    \n",
    "    def getLabels(self, label):\n",
    "        \n",
    "        if label == \"toxic\":\n",
    "            return self.toxic\n",
    "        \n",
    "        elif label == \"severe_toxic\":\n",
    "            return self.severe_toxic\n",
    "        \n",
    "        elif label == \"obscene\":\n",
    "            return self.obscene\n",
    "        \n",
    "        elif label == \"threat\":\n",
    "            return self.threat\n",
    "        \n",
    "        elif label == \"insult\":\n",
    "            return self.insult\n",
    "        \n",
    "        elif label == \"identity_hate\":\n",
    "            return self.identity_hate\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    def __init__(self, algo, nr_folds, min_c, max_c, type_):\n",
    "        \n",
    "        self.algo = algo\n",
    "        self.nr_folds = nr_folds\n",
    "        self.min_c = min_c\n",
    "        self.max_c = max_c\n",
    "        self.c_range = frange(min_c, max_c, 2)\n",
    "        self.type_ = type_\n",
    "        \n",
    "        self.best_c = 0\n",
    "        self.best_rate = 0   \n",
    "    \n",
    "    def getBestC(self, data):\n",
    "        \n",
    "        type_ = self.type_\n",
    "        features_ = data.features\n",
    "        labels_ = data.getLabels(type_)\n",
    "        \n",
    "        samples_split = generateSplits(data.num_obs, self.nr_folds)\n",
    "        print(\"done split\")\n",
    "        \n",
    "        loss_master = []\n",
    "        \n",
    "        for c in self.c_range:\n",
    "            \n",
    "            loss = []\n",
    "            \n",
    "            for i in range(0, self.nr_folds):\n",
    "                \n",
    "                test_index = samples_split[i]\n",
    "                train_index = [x for j in range(0, self.nr_folds) for x in samples_split[j] if j != i]\n",
    "                \n",
    "                train_labels = [labels_[x] for x in train_index]\n",
    "                train_features = [features_[x] for x in train_index]\n",
    "                \n",
    "                test_labels = [labels_[x] for x in test_index]\n",
    "                test_features = [features_[x] for x in test_index]\n",
    "                \n",
    "                param = \"-c \" + str(c) + \" -s \" + str(self.algo)\n",
    "                model = train(train_labels, train_features, param)\n",
    "                \n",
    "                labels, acc, proba = predict(test_labels, test_features, model, \"-b 1\")\n",
    "                \n",
    "                index = model.get_labels().index(1)\n",
    "                loss_current = loss_func(labels, [x[index] for x in proba])\n",
    "                \n",
    "                loss.extend([loss_current])\n",
    "            \n",
    "            loss_mean = np.mean(loss)\n",
    "            loss_master.extend([loss_mean])\n",
    "        \n",
    "        results_zipped = [x for x in zip(self.c_range, loss_master)]\n",
    "        results_zipped = sorted(results_zipped, key= lambda x: x[1], reverse= False)\n",
    "        \n",
    "        self.best_c = results_zipped[0][0]\n",
    "        self.best_rate = results_zipped[0][1]\n",
    "        \n",
    "        print(\"best c for %s is %f with lowest log loss of: %f \"%(label,self.best_c, self.best_rate))\n",
    "        \n",
    "    def trainModel(self, data): \n",
    "        \n",
    "        self.getBestC(data)\n",
    "        \n",
    "        type_ = self.type_\n",
    "        features_ = data.features\n",
    "        labels_ = data.getLabels(type_)\n",
    "        \n",
    "        param = \"-c \" + str(self.best_c) + \" -s \" + str(self.algo)\n",
    "        model = train(labels_, features_, param)\n",
    "        \n",
    "        print(\"Training model with label = %s\"%type_)\n",
    "        \n",
    "        self.__model = model\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        \n",
    "        features_ = test_data.features\n",
    "        labels_ = np.array([0] * len(features_))\n",
    "        \n",
    "        labels, acc, proba = predict(labels_, features_, self.__model, \"-b 1\")\n",
    "        index = self.__model.get_labels().index(1)\n",
    "\n",
    "        return [x[index] for x in proba], self.best_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algo Codes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-s type : set type of solver (default 1)\n",
    "  for multi-class classification\n",
    "     0 -- L2-regularized logistic regression (primal)\n",
    "     1 -- L2-regularized L2-loss support vector classification (dual)\n",
    "     2 -- L2-regularized L2-loss support vector classification (primal)\n",
    "     3 -- L2-regularized L1-loss support vector classification (dual)\n",
    "     4 -- support vector classification by Crammer and Singer\n",
    "     5 -- L1-regularized L2-loss support vector classification\n",
    "     6 -- L1-regularized logistic regression\n",
    "     7 -- L2-regularized logistic regression (dual)\n",
    "     \n",
    "* I work with 7 --> according to the paper, L2 regularization seems to work better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(train_data, test_data, classifier, label):\n",
    "    \n",
    "    print(\"Training for: %s\"%label)\n",
    "   \n",
    "    classifier.trainModel(train_data)\n",
    "    proba, log_loss = classifier.predict(test_data)\n",
    "    print(\"\")\n",
    "    \n",
    "    return proba, log_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for: toxic\n",
      "done split\n",
      "best c for toxic is 64.000000 with lowest log loss of: 0.071412 \n",
      "Training model with label = toxic\n",
      "\n",
      "Training for: severe_toxic\n",
      "done split\n",
      "best c for severe_toxic is 64.000000 with lowest log loss of: 0.017384 \n",
      "Training model with label = severe_toxic\n",
      "\n",
      "Training for: obscene\n",
      "done split\n",
      "best c for obscene is 64.000000 with lowest log loss of: 0.040114 \n",
      "Training model with label = obscene\n",
      "\n",
      "Training for: threat\n",
      "done split\n",
      "best c for threat is 64.000000 with lowest log loss of: 0.009765 \n",
      "Training model with label = threat\n",
      "\n",
      "Training for: insult\n",
      "done split\n",
      "best c for insult is 64.000000 with lowest log loss of: 0.048080 \n",
      "Training model with label = insult\n",
      "\n",
      "Training for: identity_hate\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #train_data = data(train_data_bow_path, train_data_manual_path, \"train\")\n",
    "    \n",
    "    #test_data = data(test_data_bow_path, test_data_manual_path, \"test\")\n",
    "    \n",
    "    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    classifiers = dict()\n",
    "    loss = []\n",
    "    \n",
    "    results = test_data.manual_df.loc[:, \"id\"]\n",
    "    \n",
    "    proba_master = []\n",
    "    \n",
    "    for label in labels:\n",
    "        \n",
    "        classifiers[label] = Classifier(0, 7, 0.03125, 64, label)\n",
    "        classifier = classifiers.get(label)\n",
    "        \n",
    "        proba_class,loss_class = main(train_data, test_data, classifier, label)\n",
    "        proba_master.extend([proba_class])\n",
    "        loss.extend([loss_class])\n",
    "        \n",
    "        results = pd.concat([results, pd.DataFrame(proba_class, columns = [label])], axis = 1)\n",
    "        \n",
    "    #results = results[\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\" ]\n",
    "    \n",
    "    results.to_csv(\"../../data/output/output - LogReg.csv\", index = False, float_format= \"%.10f\", encoding = \"utf-8\")\n",
    "    \n",
    "    print(\"Average log loss = %f\"%np.average(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
