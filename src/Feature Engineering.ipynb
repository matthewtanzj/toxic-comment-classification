{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from string import punctuation\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit Learn Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "## May want to try count vectorizer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stating the path of the data files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path of the training / testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_path = \"../data/data/train/train_cleaned.csv\"\n",
    "test_data_path = \"../data/data/test/test_cleaned.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path of the lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emo_lexicon_path = \"../data/external dataset/NRC-AffectIntensity-Lexicon.txt\"\n",
    "block_terms_path = \"../data/external dataset/Terms-To-Block.txt\"\n",
    "swear_words_path = \"../data/external dataset/swearWords.txt\"\n",
    "google_bad_words_path = \"../data/external dataset/google-bad-words.txt\"\n",
    "github_bad_words_path = \"../data/external dataset/bad-words-github.txt\"\n",
    "bad_words_path = \"../data/external dataset/bad-words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_tags(comment, tag):\n",
    "    length = len(comment.split(\" \"))\n",
    "    count = comment.lower().count(tag)\n",
    "    return float(count)/float(length), int(count > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_emotions(comment, emotion):\n",
    "    comment_array = comment.split(\" \")\n",
    "    length = len(comment_array)\n",
    "    count = len([emotion.get(x) for x in comment_array if emotion.get(x) is not None])\n",
    "    return float(count)/float(length), int(count > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_swear_words(comment, word_lst):\n",
    "#     comment = \" \" + comment + \" \"\n",
    "#     length = len(comment.split(\" \"))\n",
    "#     count = 0\n",
    "#     for word in word_lst:\n",
    "#         word = \" \" + word + \" \"\n",
    "#         if word in comment:\n",
    "#             comment = comment.replace(word, \"\")\n",
    "#             count += 1 \n",
    "    words = comment.split(\" \")\n",
    "    is_swear_word = [1 if word in word_lst else 0 for word in words]\n",
    "    total_swear_words = np.sum(is_swear_word)\n",
    "    return float(total_swear_words)/float(len(words)), int(total_swear_words>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_punc(comment):\n",
    "    counts = Counter(comment)\n",
    "    punc_dict = {key: occurences for key, occurences in counts.items() if key in punctuation}\n",
    "    punc_count = sum(punc_dict.values())\n",
    "    return punc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Emotion Lexicon class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmoLex: \n",
    "    \n",
    "    def __init__(self, emoPath):\n",
    "        self.emoPath = emoPath\n",
    "        self.emoLex = pd.read_csv(self.emoPath, sep = \"\\t\")\n",
    "        \n",
    "        self.anger = self.emoLex.loc[self.emoLex.loc[:, \"AffectDimension\"] == \"anger\", :]\n",
    "        self.fear = self.emoLex.loc[self.emoLex.loc[:, \"AffectDimension\"] == \"fear\", :]\n",
    "        self.joy = self.emoLex.loc[self.emoLex.loc[:, \"AffectDimension\"] == \"joy\", :]\n",
    "        self.sadness = self.emoLex.loc[self.emoLex.loc[:, \"AffectDimension\"] == \"sadness\", :]\n",
    "        \n",
    "        anger_dict = {}\n",
    "        for i in range(0, self.anger.shape[0]):\n",
    "            word = self.anger.iloc[i, :].term\n",
    "            score = self.anger.iloc[i, :].score\n",
    "            anger_dict[word] = score\n",
    "            \n",
    "        fear_dict = {}\n",
    "        for i in range(0, self.fear.shape[0]):\n",
    "            word = self.fear.iloc[i, :].term\n",
    "            score = self.fear.iloc[i, :].score\n",
    "            fear_dict[word] = score\n",
    "            \n",
    "        joy_dict = {}\n",
    "        for i in range(0, self.joy.shape[0]):\n",
    "            word = self.joy.iloc[i, :].term\n",
    "            score = self.joy.iloc[i, :].score\n",
    "            joy_dict[word] = score\n",
    "            \n",
    "        sadness_dict = {}\n",
    "        for i in range(0, self.sadness.shape[0]):\n",
    "            word = self.sadness.iloc[i, :].term\n",
    "            score = self.sadness.iloc[i, :].score\n",
    "            sadness_dict[word] = score\n",
    "            \n",
    "        self.anger_dict = anger_dict\n",
    "        self.fear_dict = fear_dict\n",
    "        self.joy_dict = joy_dict\n",
    "        self.sadness_dict = sadness_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the vulgarities class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vulgarities:\n",
    "    \n",
    "    def __init__(self, block_terms_path, swear_words_path, \n",
    "                 google_bad_words_path, github_bad_words_path, bad_words_path):\n",
    "        \n",
    "        self.block_terms_path = block_terms_path\n",
    "        self.swear_words_path = swear_words_path\n",
    "        self.google_bad_words_path = google_bad_words_path\n",
    "        self.github_bad_words_path = github_bad_words_path\n",
    "        self.bad_words_path = bad_words_path\n",
    "        \n",
    "#         self.block_terms_list = list(pd.read_csv(self.block_terms_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "#         self.swear_words_list = list(pd.read_csv(self.swear_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "#         self.google_bad_words_list = list(pd.read_csv(self.google_bad_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "#         self.github_bad_words_list = list(pd.read_csv(self.github_bad_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "#         self.bad_words_list = list(pd.read_csv(self.bad_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "        \n",
    "        self.block_terms_list = set(pd.read_csv(self.block_terms_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "        self.swear_words_list = set(pd.read_csv(self.swear_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "        self.google_bad_words_list = set(pd.read_csv(self.google_bad_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "        self.github_bad_words_list = set(pd.read_csv(self.github_bad_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "        self.bad_words_list = set(pd.read_csv(self.bad_words_path, header = None, names= [\"words\"]).loc[:, \"words\"])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \n",
    "    def __init__(self, path, type_):\n",
    "        self.path = path\n",
    "        self.df = pd.read_csv(path)\n",
    "    \n",
    "    def addBOWFeatures(self, vectorizer):\n",
    "        sparse_matrix = vectorizer.transform(self.df.loc[:, \"comment_text\"])\n",
    "        bow_df = pd.DataFrame(np.array(sparse_matrix.todense()))\n",
    "        self.bow = pd.concat([self.df.loc[:, \"id\"], bow_df], axis = 1)\n",
    "    \n",
    "    def addTagFeatures(self):\n",
    "        self.df.loc[:, \"pres_links\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_tags(x, \"<link>\")[1])\n",
    "        self.df.loc[:, \"pres_image\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_tags(x, \"<image>\")[1])\n",
    "        self.df.loc[:, \"pres_user\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_tags(x, \"<user>\")[1])\n",
    "        self.df.loc[:, \"pres_date\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_tags(x, \"(utc)\")[1])\n",
    "        \n",
    "    def addEmotionFeatures(self, emotion):\n",
    "        self.df.loc[:, \"pres_anger\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_emotions(x, emotion.anger_dict)[1])\n",
    "        self.df.loc[:, \"pres_fear\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_emotions(x, emotion.fear_dict)[1])\n",
    "        self.df.loc[:, \"pres_joy\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_emotions(x, emotion.joy_dict)[1])\n",
    "        self.df.loc[:, \"pres_sadness\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_emotions(x, emotion.sadness_dict)[1])\n",
    "        \n",
    "    def addVulgaritiesFeatures(self, terms):\n",
    "        self.df.loc[:, \"pres_block_terms\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_swear_words(x, terms.block_terms_list)[1])\n",
    "        self.df.loc[:, \"pres_swear_words\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_swear_words(x, terms.swear_words_list)[1])\n",
    "        self.df.loc[:, \"pres_google_bad_words\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_swear_words(x, terms.google_bad_words_list)[1])\n",
    "        self.df.loc[:, \"pres_github_bad_words\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_swear_words(x, terms.github_bad_words_list)[1])\n",
    "        self.df.loc[:, \"pres_bad_words\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: check_swear_words(x, terms.bad_words_list)[1])\n",
    "    \n",
    "    def addPunctuationCount(self):\n",
    "        self.df.loc[:, \"punc_count\"] = self.df.loc[:, \"comment_text\"].apply(lambda x: count_punc(x))\n",
    "        self.df.loc[:, \"punc_count_normed\"] = self.df.apply(lambda x: x[\"punc_count\"] / len(x[\"comment_text\"]) , axis=1)\n",
    "    \n",
    "    def writeFeatures(self, path_manual_features, path_bow_features):\n",
    "        self.bow.to_csv(path_bow_features, index = None)\n",
    "        self.df.to_csv(path_manual_features, index = None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(train_data, test_data, vectorizer):\n",
    "    \n",
    "    vulgarities_lexicon = Vulgarities(block_terms_path, swear_words_path, google_bad_words_path, github_bad_words_path, bad_words_path)\n",
    "    emo_lexicon = EmoLex(emo_lexicon_path)\n",
    "    \n",
    "    print(\"adding features for training data\")\n",
    "    train_data.addBOWFeatures(vectorizer)\n",
    "    train_data.addTagFeatures()\n",
    "    train_data.addVulgaritiesFeatures(vulgarities_lexicon)\n",
    "    train_data.addEmotionFeatures(emo_lexicon)\n",
    "    train_data.addPunctuationCount()\n",
    "    train_data.performSentimentAnalysis()\n",
    "    train_data.writeFeatures(\"../data/data/train/train_data_manual.csv\", \"../data/data/train/train_data_bow.csv\")\n",
    "    print(\"done adding features for training data\")\n",
    "    \n",
    "    print(\"adding features for testing data\")\n",
    "    test_data.addBOWFeatures(vectorizer)\n",
    "    test_data.addTagFeatures()\n",
    "    test_data.addVulgaritiesFeatures(vulgarities_lexicon)\n",
    "    test_data.addEmotionFeatures(emo_lexicon)\n",
    "    test_data.addPunctuationCount()\n",
    "    test_data.writeFeatures(\"../data/data/test/test_data_manual.csv\", \"../data/data/test/test_data_bow.csv\")\n",
    "    print(\"done adding features for testing data\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train_data = Data(train_data_path, \"train\")\n",
    "    test_data = Data(test_data_path, \"test\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df= 0.05, max_df = 0.75, lowercase=True, ngram_range= (1,2)) # play around with the ngram\n",
    "    comments = list(train_data.df.loc[:, \"comment_text\"]) + list(test_data.df.loc[:, \"comment_text\"])\n",
    "    vectorizer.fit(comments)\n",
    "    \n",
    "    train_data, test_data = main(train_data, test_data, vectorizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "afinn = Afinn()\n",
    "train_data = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense? kiss off, geek. what I said is true....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\" Please do not vandalize pages, as you did wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\" \"\"Points of interest\"\" I removed the \"\"point...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense? kiss off, geek. what I said is true....      1   \n",
       "1  27450690  \" Please do not vandalize pages, as you did wi...      0   \n",
       "2  54037174  \" \"\"Points of interest\"\" I removed the \"\"point...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.loc[:, \"sentiment_val\"] = train_data.loc[:, \"comment_text\"].apply(lambda x: afinn.score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sentiment_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense? kiss off, geek. what I said is true....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\" Please do not vandalize pages, as you did wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\" \"\"Points of interest\"\" I removed the \"\"point...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>82428052</td>\n",
       "      <td>Fried chickens Is dat sum fried chickens?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>87311443</td>\n",
       "      <td>Why can you put English for example on some pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>114749757</td>\n",
       "      <td>Guy Fawkes im a resident in bridgwater and i g...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>138560519</td>\n",
       "      <td>as far as nicknames go this article is embarra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>139353149</td>\n",
       "      <td>Woodland Meadows Good to hear that you correct...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150009866</td>\n",
       "      <td>\" Well I just finished a good bit of editing. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                       comment_text  toxic  \\\n",
       "0    22256635  Nonsense? kiss off, geek. what I said is true....      1   \n",
       "1    27450690  \" Please do not vandalize pages, as you did wi...      0   \n",
       "2    54037174  \" \"\"Points of interest\"\" I removed the \"\"point...      0   \n",
       "3    77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4    79357270  The reader here is not going by my say so for ...      0   \n",
       "5    82428052          Fried chickens Is dat sum fried chickens?      0   \n",
       "6    87311443  Why can you put English for example on some pl...      0   \n",
       "7   114749757  Guy Fawkes im a resident in bridgwater and i g...      0   \n",
       "8   138560519  as far as nicknames go this article is embarra...      0   \n",
       "9   139353149  Woodland Meadows Good to hear that you correct...      0   \n",
       "10  150009866  \" Well I just finished a good bit of editing. ...      0   \n",
       "\n",
       "    severe_toxic  obscene  threat  insult  identity_hate  sentiment_val  \n",
       "0              0        0       0       0              0            2.0  \n",
       "1              0        0       0       0              0            0.0  \n",
       "2              0        0       0       0              0            4.0  \n",
       "3              0        0       0       0              0            3.0  \n",
       "4              0        0       0       0              0            0.0  \n",
       "5              0        0       0       0              0            0.0  \n",
       "6              0        0       0       0              0            2.0  \n",
       "7              0        0       0       0              0            4.0  \n",
       "8              0        0       0       0              0           -3.0  \n",
       "9              0        0       0       0              0            3.0  \n",
       "10             0        0       0       0              0           14.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" Well I just finished a good bit of editing. I haven\\'t had a chance to go through and do all the tagging and referencing yet since work calls but I am extremely confident in the scientific factual basis of everything stated in here. The referencing can pretty much be all made to the book \"\"Particle Imaging Velocimetry A practical guide\"\" by Raffel, Willert, Wereley and Kompenhans if someone can add it. Also, not sure how to exactly do that as it all can be found in numerous scientific journals, databases, etc. but is so conveniently placed all in that one book. The only thing I am unsure about as I have no experience with is Molecular Tagging Velocimetry. To put it simply, I would like to keep technique defects in the summaries but I don\\'t know what this one\\'s is. To put is bluntly, the description as it is now is too good to be true. I know it has defects and problems, I don\\'t know what they are. The fact is the technique was developed in 97ish and is not in widespread use. If the description were accurate, everyone would use it. I have met or heard of not one person using it, which means there is some big flaw in the technique that if someone can find, be my guest to add it. Everything else, please post on discussion page before editing critical information as I am positive what I have said can be completely defended but am open to solid arguments. I won\\'t be pigheaded about it. Also, anyone with knowledge of tomographic PIV please expand the section. It is a volumetric analysis technique that I know of and know its new, but know nothing about.\"'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"You are a great great great person! What is wrong with you!! What is wrong with you!!!!! Very good review! Nice and friendly place with excellent food and friendly and helpful staff. You need a car though. The children wants to go back! Playground and animals entertained them and they felt like at home. I also recommend the dinner! Great value for the price!\"\n",
    "sentences = nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stokens = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You', 'are', 'a', 'great', 'great', 'great', 'person', '!'],\n",
       " ['What', 'is', 'wrong', 'with', 'you', '!', '!'],\n",
       " ['What', 'is', 'wrong', 'with', 'you', '!', '!', '!', '!', '!'],\n",
       " ['Very', 'good', 'review', '!'],\n",
       " ['Nice',\n",
       "  'and',\n",
       "  'friendly',\n",
       "  'place',\n",
       "  'with',\n",
       "  'excellent',\n",
       "  'food',\n",
       "  'and',\n",
       "  'friendly',\n",
       "  'and',\n",
       "  'helpful',\n",
       "  'staff',\n",
       "  '.'],\n",
       " ['You', 'need', 'a', 'car', 'though', '.'],\n",
       " ['The', 'children', 'wants', 'to', 'go', 'back', '!'],\n",
       " ['Playground',\n",
       "  'and',\n",
       "  'animals',\n",
       "  'entertained',\n",
       "  'them',\n",
       "  'and',\n",
       "  'they',\n",
       "  'felt',\n",
       "  'like',\n",
       "  'at',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['I', 'also', 'recommend', 'the', 'dinner', '!'],\n",
       " ['Great', 'value', 'for', 'the', 'price', '!']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('You', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('great', 'JJ'), ('great', 'JJ'), ('great', 'JJ'), ('person', 'NN'), ('!', '.')], [('What', 'WP'), ('is', 'VBZ'), ('wrong', 'JJ'), ('with', 'IN'), ('you', 'PRP'), ('!', '.'), ('!', '.')], [('What', 'WP'), ('is', 'VBZ'), ('wrong', 'JJ'), ('with', 'IN'), ('you', 'PRP'), ('!', '.'), ('!', '.'), ('!', '.'), ('!', '.'), ('!', '.')], [('Very', 'RB'), ('good', 'JJ'), ('review', 'NN'), ('!', '.')], [('Nice', 'NNP'), ('and', 'CC'), ('friendly', 'JJ'), ('place', 'NN'), ('with', 'IN'), ('excellent', 'JJ'), ('food', 'NN'), ('and', 'CC'), ('friendly', 'JJ'), ('and', 'CC'), ('helpful', 'JJ'), ('staff', 'NN'), ('.', '.')], [('You', 'PRP'), ('need', 'VBP'), ('a', 'DT'), ('car', 'NN'), ('though', 'NN'), ('.', '.')], [('The', 'DT'), ('children', 'NNS'), ('wants', 'VBZ'), ('to', 'TO'), ('go', 'VB'), ('back', 'RB'), ('!', '.')], [('Playground', 'NN'), ('and', 'CC'), ('animals', 'NNS'), ('entertained', 'VBD'), ('them', 'PRP'), ('and', 'CC'), ('they', 'PRP'), ('felt', 'VBD'), ('like', 'IN'), ('at', 'IN'), ('home', 'NN'), ('.', '.')], [('I', 'PRP'), ('also', 'RB'), ('recommend', 'VBP'), ('the', 'DT'), ('dinner', 'NN'), ('!', '.')], [('Great', 'NNP'), ('value', 'NN'), ('for', 'IN'), ('the', 'DT'), ('price', 'NN'), ('!', '.')]]\n"
     ]
    }
   ],
   "source": [
    "taggedlist = []\n",
    "for stoken in stokens:        \n",
    "     taggedlist.append(nltk.pos_tag(stoken))\n",
    "print(taggedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.009615384615384616, 0.2916666666666667, 0.2916666666666667, 0.2916666666666667, 0.0], [0.009615384615384616, -0.5972222222222222], [0.009615384615384616, -0.5972222222222222], [0.125, 0.6130952380952381, 0.025], [0.0, 0.1875, 0.0, 1.0, -0.041666666666666664, 0.1875, 0.25, 0.0], [-0.08333333333333333, 0.0], [0.03125, -0.075, 0.0125, 0.0], [0.0, -0.375, 0.08333333333333333, 0.015625, 0.027777777777777776], [0.0, 0.25, 0.0], [0.0, 0.10416666666666667, 0.08928571428571429]]\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "score_list=[]\n",
    "for idx,taggedsent in enumerate(taggedlist):\n",
    "    score_list.append([])\n",
    "    for idx2,t in enumerate(taggedsent):\n",
    "        newtag=''\n",
    "        lemmatized=wnl.lemmatize(t[0])\n",
    "        if t[1].startswith('NN'):\n",
    "            newtag='n'\n",
    "        elif t[1].startswith('JJ'):\n",
    "            newtag='a'\n",
    "        elif t[1].startswith('V'):\n",
    "            newtag='v'\n",
    "        elif t[1].startswith('R'):\n",
    "            newtag='r'\n",
    "        else:\n",
    "            newtag=''       \n",
    "        if(newtag!=''):    \n",
    "            synsets = list(swn.senti_synsets(lemmatized, newtag))\n",
    "            #Getting average of all possible sentiments, as you requested        \n",
    "            score=0\n",
    "            if(len(synsets)>0):\n",
    "                for syn in synsets:\n",
    "                    score+=syn.pos_score()-syn.neg_score()\n",
    "                score_list[idx].append(score/len(synsets))\n",
    "            \n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for each sentence for:You are a great great great person! What is wrong with you!! What is wrong with you!!!!! Very good review! Nice and friendly place with excellent food and friendly and helpful staff. You need a car though. The children wants to go back! Playground and animals entertained them and they felt like at home. I also recommend the dinner! Great value for the price!\n",
      "\n",
      "[0.17692307692307696, -0.2938034188034188, -0.2938034188034188, 0.2543650793650794, 0.19791666666666666, -0.041666666666666664, -0.007812499999999999, -0.04965277777777778, 0.08333333333333333, 0.06448412698412699]\n"
     ]
    }
   ],
   "source": [
    "sentence_sentiment=[]\n",
    "\n",
    "for score_sent in score_list:\n",
    "    sentence_sentiment.append(sum([word_score for word_score in score_sent])/len(score_sent))\n",
    "print(\"Sentiment for each sentence for:\" + data + \"\\n\")\n",
    "print(sentence_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a great great great person!                                             0.17692307692307696\n",
      "What is wrong with you!!                                                        -0.2938034188034188\n",
      "What is wrong with you!!!!!                                                     -0.2938034188034188\n",
      "Very good review!                                                               0.2543650793650794\n",
      "Nice and friendly place with excellent food and friendly and helpful staff.     0.19791666666666666\n",
      "You need a car though.                                                          -0.041666666666666664\n",
      "The children wants to go back!                                                  -0.007812499999999999\n",
      "Playground and animals entertained them and they felt like at home.             -0.04965277777777778\n",
      "I also recommend the dinner!                                                    0.08333333333333333\n",
      "Great value for the price!                                                      0.06448412698412699\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(sentences)):\n",
    "    print(sentences[index].ljust(80) + str(sentence_sentiment[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Sentiment ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"Congratulations. You are a great! You suck! Asshole! Fucker! Fuck. Bitches. Gay. Sissy. Motherfuckers. PENIS. Fker. a$$hole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Congratulations', 'NNS'), ('.', '.'), ('You', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('great', 'JJ'), ('!', '.'), ('You', 'PRP'), ('suck', 'VBP'), ('!', '.'), ('Asshole', 'NN'), ('!', '.'), ('Fucker', 'NN'), ('!', '.'), ('Fuck', 'NNP'), ('.', '.'), ('Bitches', 'NNP'), ('.', '.'), ('Gay', 'NNP'), ('.', '.'), ('Sissy', 'NNP'), ('.', '.'), ('Motherfuckers', 'NNP'), ('.', '.'), ('PENIS', 'NNP'), ('.', '.'), ('Fker', 'NNP'), ('.', '.'), ('a', 'DT'), ('$', '$'), ('$', '$'), ('hole', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "taggedlist = []    \n",
    "taggedlist.append(nltk.pos_tag(tokens))\n",
    "print(taggedlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations n\n",
      "are v\n",
      "great a\n",
      "suck v\n",
      "Asshole n\n",
      "Fucker n\n",
      "Fuck n\n",
      "Bitches n\n",
      "Gay n\n",
      "Sissy n\n",
      "Motherfuckers n\n",
      "PENIS n\n",
      "hole n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Congratulations',\n",
       " 'are',\n",
       " 'great',\n",
       " 'suck',\n",
       " 'Asshole',\n",
       " 'Fucker',\n",
       " 'Fuck',\n",
       " 'Bitches',\n",
       " 'Gay',\n",
       " 'Sissy',\n",
       " 'Motherfuckers',\n",
       " 'PENIS',\n",
       " 'hole']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "score_list=[]\n",
    "word_list=[]\n",
    "for idx,t in enumerate(taggedlist[0]):\n",
    "    newtag=''\n",
    "    lemmatized=wnl.lemmatize(t[0])\n",
    "    if t[1].startswith('NN'):\n",
    "        newtag='n'\n",
    "    elif t[1].startswith('JJ'):\n",
    "        newtag='a'\n",
    "    elif t[1].startswith('V'):\n",
    "        newtag='v'\n",
    "    elif t[1].startswith('R'):\n",
    "        newtag='r'\n",
    "    else:\n",
    "        newtag=''\n",
    "    if(newtag!=''):\n",
    "        synsets = list(swn.senti_synsets(lemmatized, newtag))        \n",
    "        score=0\n",
    "        if(len(synsets)>0):\n",
    "            print(lemmatized, newtag)\n",
    "            for syn in synsets:\n",
    "                score+=syn.pos_score()-syn.neg_score()\n",
    "            score_list.append(score/len(synsets))\n",
    "            word_list.append(lemmatized)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations     0.5416666666666666\n",
      "are                 0.009615384615384616\n",
      "great               0.2916666666666667\n",
      "suck                -0.07142857142857142\n",
      "Asshole             -0.375\n",
      "Fucker              -0.0625\n",
      "Fuck                0.0\n",
      "Bitches             -0.1875\n",
      "Gay                 0.125\n",
      "Sissy               0.0\n",
      "Motherfuckers       -0.625\n",
      "PENIS               0.0\n",
      "hole                -0.078125\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(word_list)):\n",
    "    print(word_list[index].ljust(20) + str(score_list[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03320037334460412\n"
     ]
    }
   ],
   "source": [
    "sentence_sentiment= sum(score for score in score_list)/len(score_list)\n",
    "\n",
    "print(sentence_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations. You are a great! You suck! Asshole! Fucker! Fuck. Bitches. Gay. Sissy. Motherfuckers. PENIS. Fker. a$$hole\n",
      "-0.03320037334460412\n"
     ]
    }
   ],
   "source": [
    "print(data.ljust(80))\n",
    "print(str(sentence_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
