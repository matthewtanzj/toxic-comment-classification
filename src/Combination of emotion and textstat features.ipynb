{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textstat.textstat import textstat\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import make_scorer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the liblinear path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing sklearn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stating the path of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"../data/data/train.csv\"\n",
    "test_path = \"../data/data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_lexicon_path = \"../data/features/NRC-AffectIntensity-Lexicon.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_image = re.compile(\"((\\S*)|((Image:|File:).*))(.gif|.png|.tiff|.jpg|.jpeg)\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_links = re.compile(\"((http)|(www))\\S*\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_new_line = re.compile(\"\\n\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_multiple_white_space = re.compile(\"\\s+\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_users = re.compile(\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_user1 = re.compile(\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\", re.IGNORECASE)\n",
    "regex_user2 = re.compile(\"\\[{1,2}(User:|User Talk:|User talk:|User_talk:)\\s?\\w*(\\||\\]{1,2})?\", re.IGNORECASE)\n",
    "\n",
    "regex_list_user = [regex_user1, regex_user2]\n",
    "\n",
    "def replace_user_multiple_regex(comment, regexList):\n",
    "    for regex in regexList:\n",
    "        comment = regex.sub(\"<user>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_new_line(comment, regex):\n",
    "    comment = regex.sub(\" \", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_image(comment, regex):\n",
    "    comment = regex.sub(\"<image>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_links(comment, regex):\n",
    "    comment = regex.sub(\"<link>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_mutiple_white_space(comment, regex):\n",
    "    comment = regex.sub(\" \", comment).strip()\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_user(comment, regex):\n",
    "    comment = regex.sub(\"<user>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      " Edit war brewing at Yosemite National Park \n",
      "\n",
      "If you have some spare energy, please come and lend a hand. Thanks! —  \n",
      "\n",
      "Looked ... acted ... )   \n",
      "\n",
      "I was also approached recently with regards to possible edits to the Yosemite National Park article. My input on the matter can be found at the following link;\n",
      "user talk:Edit Centric#Hantavirus Risk in Yosemite\n",
      "\n",
      "Noted,   \n",
      "\n",
      " Thmc1 / 173.63.176.93 sill sockpuppeting / REPLYU REQUSTED \n",
      "\n",
      "This person (blocked sockmaster Thmc1) is still block evading by not logging in, even after he was warned by SysAdmin Kudpung. I would suggest that you block the suspected IPs(173.63176.93, 74.88.160.244) as well as IPs used by his other blocked account (Nyc88) and any other accounts affiliated w/them. Why are sockppuppets of a blocked sockmaster still allowed to edit even after ample warnings? To make matters worse, he even violated his block and continuted editing the day after he was blocked! Please see Thmc1 sockpuppet investigation archive for latest investigation/evidence.   \n",
      "\n",
      "No clue  links would be helpful.   \n",
      "\n",
      "Actually I think it's the other way around, if I'm not mistaken, I think I remember   being mentioned to admin Ronhjones as a possible sockpuppet in the past few months in conjunction with an edit disagreement issue. Are you stocking this address? Is this some sort of retaliation? First of all, I have no idea who any of these other users are, much less others who use this IP address. 173.63.176.93  \n",
      "Hmm... Thanks, I think, hadn't noticed the 173... above was the same as the 173... I just talked to .. still no clue why MBaxter showed up here. Aw well - keep on keepin' on   \n",
      "\n",
      "Vsmith- Don't let 173.63.176.93 fool you. It was him who attempted to mention this to RonHJones using IP 74.88.160.244. That IP was found to be one of 173.63.176.93's sockpuppet and blocked after a recent investigation. 173.63.176.93 is actually a sockpuppet of Thmc1, blocked for excessive sockpuppeting back in 2010. Even though 173.63.176.93 was ruled most likley as one of Thmc1's sockpuppet, he was let off with a warning. For the full story, I suggest that you check out Thmc1's most recent investigation, August 2012. The final assessment was one account (Nyc88) permanently blocked, one IP temporarily blocked, and one left open with warning. If he causes you anymore trouble, you should report this to the SysAdmins and don't hesitate to mention the sockpuppet investigation. The reverting of your edits is the same kind of BS he conducted on the London City article that got him blocked back in 2010. He has an anti-UK bias amongst many, and you can confirm this with user Eraserhead1.   —Preceding undated comment added  \n",
      "\n",
      "Hantavirus\n",
      "VSmith, I actually came to your talk page for a different reason and just happened to notice this. It looks ridiculous and I think this MBaxter1 character is highly suspicious himself. In the meantime, can you please look at this story seen on the main CNN.com page today? http://www.cnn.com/2012/09/05/health/hantavirus-warnings/index.html?hpt=hp_t1\n",
      "I think that this Yosemite hantavirus story is worthy of at least brief mention in the article because it is now having international implications. I just don't see a justification in ignoring it completely, thanks.173.63.176.93  \n",
      "\n",
      "Does the article have a current events section? The hantavirus bit isn't history, and as it is notable, it should be discussed on the hantavirus page ... haven't checked. And, no, don't think we need a current events section on the page ... WP:notnews.  ([[User talk:Vsmi\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\" Edit war brewing at Yosemite National Park If you have some spare energy, please come and lend a hand. Thanks! — Looked ... acted ... ) I was also approached recently with regards to possible edits to the Yosemite National Park article. My input on the matter can be found at the following link; user talk:Edit Centric#Hantavirus Risk in Yosemite Noted, Thmc1 / <user> sill sockpuppeting / REPLYU REQUSTED This person (blocked sockmaster Thmc1) is still block evading by not logging in, even after he was warned by SysAdmin Kudpung. I would suggest that you block the suspected IPs(173.63176.93, <user>) as well as IPs used by his other blocked account (Nyc88) and any other accounts affiliated w/them. Why are sockppuppets of a blocked sockmaster still allowed to edit even after ample warnings? To make matters worse, he even violated his block and continuted editing the day after he was blocked! Please see Thmc1 sockpuppet investigation archive for latest investigation/evidence. No clue links would be helpful. Actually I think it's the other way around, if I'm not mistaken, I think I remember being mentioned to admin Ronhjones as a possible sockpuppet in the past few months in conjunction with an edit disagreement issue. Are you stocking this address? Is this some sort of retaliation? First of all, I have no idea who any of these other users are, much less others who use this IP address. <user> Hmm... Thanks, I think, hadn't noticed the 173... above was the same as the 173... I just talked to .. still no clue why MBaxter showed up here. Aw well - keep on keepin' on Vsmith- Don't let <user> fool you. It was him who attempted to mention this to RonHJones using IP <user>. That IP was found to be one of <user>'s sockpuppet and blocked after a recent investigation. <user> is actually a sockpuppet of Thmc1, blocked for excessive sockpuppeting back in 2010. Even though <user> was ruled most likley as one of Thmc1's sockpuppet, he was let off with a warning. For the full story, I suggest that you check out Thmc1's most recent investigation, August 2012. The final assessment was one account (Nyc88) permanently blocked, one IP temporarily blocked, and one left open with warning. If he causes you anymore trouble, you should report this to the SysAdmins and don't hesitate to mention the sockpuppet investigation. The reverting of your edits is the same kind of BS he conducted on the London City article that got him blocked back in 2010. He has an anti-UK bias amongst many, and you can confirm this with user Eraserhead1. —Preceding undated comment added Hantavirus VSmith, I actually came to your talk page for a different reason and just happened to notice this. It looks ridiculous and I think this MBaxter1 character is highly suspicious himself. In the meantime, can you please look at this story seen on the main CNN.com page today? <link> I think that this Yosemite hantavirus story is worthy of at least brief mention in the article because it is now having international implications. I just don't see a justification in ignoring it completely, thanks.<user> Does the article have a current events section? The hantavirus bit isn't history, and as it is notable, it should be discussed on the hantavirus page ... haven't checked. And, no, don't think we need a current events section on the page ... WP:notnews. (<user>\n"
     ]
    }
   ],
   "source": [
    "#for index in [8419, 8417, 3695]:\n",
    "for index in [3695]:\n",
    "    trial = trainData.iloc[index,1]\n",
    "    print(trial)\n",
    "    print(\"-----------------------------------------------------------------------------------------------\")\n",
    "    trial = remove_new_line(trial, regex_new_line)\n",
    "    trial = remove_mutiple_white_space(trial, regex_multiple_white_space)\n",
    "    trial = replace_links(trial, regex_links)\n",
    "    trial = replace_user_multiple_regex(trial, regex_list_user)\n",
    "    print(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = pd.read_csv(train_path)\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Image:John Holmes arrested.jpg listed for deletion \\nAn image or media file that you uploaded or altered, Image:John Holmes arrested.jpg, has been listed at Wikipedia:Images and media for deletion. Please see the discussion to see why this is (you may have to search for the title of the image to find its entry), if you are interested in it not being deleted.  —gr'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial1 = trainData.iloc[67,1]\n",
    "trial1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_comment(comment): # cleaning to be filled up\n",
    "    if math.isnan(comment):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textstat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_syllable(comment):\n",
    "    comment_lst = comment.split(\" \")\n",
    "    syllable_count = []\n",
    "    for word in comment_lst:\n",
    "        try:\n",
    "            syllable_count.extend([textstat.syllable_count(word)])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(syllable_count) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.average(syllable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Translator to remove all puncutuations "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* I did not remove all the punctuations at the data cleaning stage because: \n",
    "    - some of the puncutuations may be useful (ie, emoticons / intensifiers like ?!?!?!?) \n",
    "  \n",
    "  hence, these can be captured during feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get the average emotion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_emotion(comment, dict_):\n",
    "    try:\n",
    "        comment = comment.translate(translator)\n",
    "    except:\n",
    "        comment = \"\"\n",
    "    comment_lst = comment.split(\" \")\n",
    "    score_lst = [dict_.get(word.lower()) for word in comment_lst if dict_.get(word.lower()) is not None]\n",
    "    if len(score_lst) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.average(score_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_file(comment):\n",
    "    extensions = [\".jpg\", \".png\", \".jpeg\", \".gif\", \"tiff\"]\n",
    "    \n",
    "    for extension in extensions: \n",
    "        \n",
    "        if extension in comment.lower():\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data File Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFile:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.__path = path\n",
    "        df = pd.read_csv(path).replace(np.nan, '', regex=True)\n",
    "        self.__df = df\n",
    "        \n",
    "    def dataCleaning(self):\n",
    "        self.__df.loc[:, \"comment_text\"] = self.__df.loc[:, \"comment_text\"].apply(lambda x : clean_comment(x) )\n",
    "    \n",
    "    def getDF(self):\n",
    "        return self.__df\n",
    "    \n",
    "    def updateDF(self, new_df):\n",
    "        self.__df = new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeaturesEngineer:\n",
    "    \n",
    "    def __init__(self, emotion_lexicon_path):\n",
    "        self.emotion_lexicon_path = emotion_lexicon_path\n",
    "        self.emotion_lexicon = pd.read_csv(self.emotion_lexicon_path, sep = \"\\t\")\n",
    "        \n",
    "        self.anger = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"anger\", :]\n",
    "        self.fear = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"fear\", :]\n",
    "        self.joy = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"joy\", :]\n",
    "        self.sadness = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"sadness\", :]\n",
    "        \n",
    "        anger_dict = {}\n",
    "        for i in range(0, self.anger.shape[0]):\n",
    "            word = self.anger.iloc[i, :].term\n",
    "            score = self.anger.iloc[i, :].score\n",
    "            anger_dict[word] = score\n",
    "            \n",
    "        fear_dict = {}\n",
    "        for i in range(0, self.fear.shape[0]):\n",
    "            word = self.fear.iloc[i, :].term\n",
    "            score = self.fear.iloc[i, :].score\n",
    "            fear_dict[word] = score\n",
    "            \n",
    "        joy_dict = {}\n",
    "        for i in range(0, self.joy.shape[0]):\n",
    "            word = self.joy.iloc[i, :].term\n",
    "            score = self.joy.iloc[i, :].score\n",
    "            joy_dict[word] = score\n",
    "            \n",
    "        sadness_dict = {}\n",
    "        for i in range(0, self.sadness.shape[0]):\n",
    "            word = self.sadness.iloc[i, :].term\n",
    "            score = self.sadness.iloc[i, :].score\n",
    "            sadness_dict[word] = score\n",
    "            \n",
    "        self.anger_dict = anger_dict\n",
    "        self.fear_dict = fear_dict\n",
    "        self.joy_dict = joy_dict\n",
    "        self.sadness_dict = sadness_dict\n",
    "    \n",
    "    def createTextStatFeatures(self, df):\n",
    "        \n",
    "        df.loc[:,'comment_len'] = df.loc[:,'comment_text'].apply(lambda x: len(x))\n",
    "        df.loc[:,'comment_avg_syllable'] = df.loc[:,'comment_text'].apply(lambda x: average_syllable(x))\n",
    "        df.loc[:,'comment_syllable'] = df.loc[:,'comment_text'].apply(lambda x: textstat.syllable_count(x))\n",
    "        df.loc[:,'comment_num_sent'] = df.loc[:, 'comment_text'].apply(lambda x: textstat.sentence_count(x))\n",
    "        df.loc[:, \"comment_word_per_sent\"] = df.loc[:, \"comment_text\"].apply(lambda x: textstat.lexicon_count(x) / textstat.sentence_count(x))\n",
    "        df.loc[:,'comment_flesch_reading_ease'] = df.loc[:,'comment_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "        df.loc[:,'comment_flesch_kincaid_grade'] = df.loc[:,'comment_text'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def createEmotionFeatures(self, df):\n",
    "        \n",
    "        df.loc[:, \"avg_anger\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.anger_dict))\n",
    "        df.loc[:, \"avg_fear\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.fear_dict))\n",
    "        df.loc[:, \"avg_joy\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.joy_dict))\n",
    "        df.loc[:, \"avg_sadness\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.sadness_dict))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def booleanFeatures()\n",
    "    \n",
    "    def intensifiersFeatures(self, df):\n",
    "        \n",
    "        df.loc[:, \"emphasized_words\"] = df.loc[:, \"comment_text\"].apply(lambda x: x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_func(y_label, y_proba):\n",
    "    return log_loss(y_label, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the engineered features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_manual_features(df):\n",
    "    \n",
    "    col = [\"id\", \"comment_text\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\" ]\n",
    "    df_sieved = df.drop(col, axis = 1)\n",
    "    \n",
    "    return df_sieved\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning to FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_manual_features_ft = FunctionTransformer(get_manual_features, validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do grid search for the best C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data in nr-fold cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(num, cv):\n",
    "    \n",
    "    num_points = int(num/cv)\n",
    "    num_points_last = num - (cv-1) * num_points\n",
    "    selected = []\n",
    "    index_test = []\n",
    "    index_train = []\n",
    "    \n",
    "    for i in range(0, cv):\n",
    "        if i == cv-1:\n",
    "            choices = [x for x in range(0, num) if x not in selected]\n",
    "            picks = random.sample(choices, num_points_last)\n",
    "            remaining = [x for x in range(0, num) if x not in picks]\n",
    "            index_test.append(picks)\n",
    "            index_train.append(remaining)\n",
    "            selected.extend(picks)\n",
    "        else:\n",
    "            choices = [x for x in range(0, num) if x not in selected]\n",
    "            picks = random.sample(choices, num_points)\n",
    "            remaining = [x for x in range(0, num) if x not in picks]\n",
    "            index_test.append(picks)\n",
    "            index_train.append(remaining)\n",
    "            selected.extend(picks)\n",
    "            \n",
    "    return index_train, index_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_C(min_c, max_c, step, data, label, cv):\n",
    "    index_train, index_test = split_data(data.shape[0], cv)\n",
    "    c_range = np.arange(min_c, max_c, step)\n",
    "    loss_lst_compiled = []\n",
    "    for c in c_range:\n",
    "        val = 2.0 ** c\n",
    "        loss_lst = []\n",
    "        for i in range(0, cv):\n",
    "            test_ = index_test[i]\n",
    "            train_ = index_train[i]\n",
    "            \n",
    "            features_train = data.iloc[train_, :]\n",
    "            labels_train = data.iloc[train_, :].loc[:, label]\n",
    "            \n",
    "            features_test = data.iloc[test_, :]\n",
    "            labels_test = data.iloc[test_, :].loc[:, label]\n",
    "            \n",
    "            param = \"-c \" + str(val) + \" -s \" + str(self.algo)\n",
    "    \n",
    "            pipe.fit(features_train, labels_train)\n",
    "            labels = pipe.predict(features_test)\n",
    "            proba = pipe.predict_proba(features_test)\n",
    "            \n",
    "            loss = loss_func(labels_test, proba)\n",
    "            loss_lst.extend([loss])\n",
    "        \n",
    "        avg_loss = np.average(loss_lst)\n",
    "        loss_lst_compiled.append(avg_loss)\n",
    "        \n",
    "    loss_lst_compiled = [x for x in zip(c_range, loss_lst_compiled)]  \n",
    "    loss_lst_compiled = sorted(loss_lst_compiled, key= lambda x: x[1])\n",
    "    return  loss_lst_compiled[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    loss_compiled = []\n",
    "    \n",
    "    for label in labels:\n",
    "        best_c, best_rate = best_C(-5,5,1, trainData.getDF(), label, 5)\n",
    "        print('Log loss for \"{}\" based on 5 fold CV: {}, with log2c = {}'.format(label, best_rate, best_c))\n",
    "        loss_compiled.extend([best_rate])\n",
    "    \n",
    "    print(loss_compiled)\n",
    "    avg_loss = np.average(loss_compiled)\n",
    "    print('Average Log loss = {}'.format(avg_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #extractor = FeaturesEngineer(emotion_lexicon_path)\n",
    "    #trainData = DataFile(path = train_path)\n",
    "    print(\"Done reading in train data\")\n",
    "    #testData = DataFile(path= test_path)\n",
    "    #print(\"Done reading in test data\")\n",
    "    \n",
    "    #trainData.updateDF(extractor.createEmotionFeatures(trainData.getDF()))\n",
    "    print(\"Done creating emotion features for train data\")\n",
    "    #testData.updateDF(extractor.createEmotionFeatures(testData.getDF()))\n",
    "    #print(\"Done creating emotion features for test data\")\n",
    "    \n",
    "    #trainData.updateDF(extractor.createTextStatFeatures(trainData.getDF()))\n",
    "    print(\"Done creating textstat features for train data\")\n",
    "    #testData.updateDF(extractor.createTextStatFeatures(testData.getDF()))\n",
    "    #print(\"Done creating textstat features for test data\")\n",
    "    \n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "200 max iterations: \n",
    "Log loss for \"toxic\" based on 5 fold CV: 0.2888927887844984, with log2c = 0\n",
    "Log loss for \"severe_toxic\" based on 5 fold CV: 0.052191609111000556, with log2c = 3\n",
    "Log loss for \"obscene\" based on 5 fold CV: 0.19248318827050923, with log2c = -1\n",
    "Log loss for \"threat\" based on 5 fold CV: 0.016697640474759634, with log2c = 4\n",
    "Log loss for \"insult\" based on 5 fold CV: 0.1815360776373493, with log2c = 0\n",
    "Log loss for \"identity_hate\" based on 5 fold CV: 0.04793890910190407, with log2c = -1\n",
    "Average Log loss = 0.12995670223000352"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "100 max_iterations: \n",
    "\n",
    "Log loss for \"toxic\" based on 5 fold CV: 0.28947272723407264, with log2c = 2\n",
    "Log loss for \"severe_toxic\" based on 5 fold CV: 0.05202002362824583, with log2c = 2\n",
    "Log loss for \"obscene\" based on 5 fold CV: 0.19150573839222035, with log2c = 2\n",
    "Log loss for \"threat\" based on 5 fold CV: 0.01670371289983063, with log2c = 1\n",
    "Log loss for \"insult\" based on 5 fold CV: 0.1808462941673421, with log2c = 2\n",
    "Log loss for \"identity_hate\" based on 5 fold CV: 0.04778838394220845, with log2c = 1\n",
    "Average Log loss = 0.12972281337731997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData.getDF().to_csv(\"../data/features/train_data_cache.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression(max_iter=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
