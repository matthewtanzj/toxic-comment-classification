{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textstat.textstat import textstat\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import make_scorer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the liblinear path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing sklearn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stating the path of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"../data/data/train.csv\"\n",
    "test_path = \"../data/data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = pd.read_csv(train_path)\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData = pd.read_csv(test_path)\n",
    "testData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_lexicon_path = \"../data/features/NRC-AffectIntensity-Lexicon.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_image1 = re.compile(\"((\\S*)|((Image:|File:).*))(.gif|.png|.tiff|.jpg|.jpeg|.svg|.ogg|.pdf|.m4a)\", re.IGNORECASE)\n",
    "regex_image2 = re.compile(\"(?i)(\\[wiki_link: image.*\\])\", re.IGNORECASE)\n",
    "regex_list_image = [regex_image1, regex_image2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_links1 = re.compile(\"(?i)(\\[wiki_link:.*?\\])\", re.IGNORECASE)\n",
    "regex_links2 = re.compile(\"(?i)(\\[EXTERNA(L)?_LINK:.*?\\])\", re.IGNORECASE)\n",
    "regex_links3 = re.compile(\"((http)|(www))\\S*\", re.IGNORECASE)\n",
    "regex_list_link = [regex_links1, regex_links2, regex_links3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_new_line = re.compile(\"\\n\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_multiple_white_space = re.compile(\"\\s+\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_user1 = re.compile(\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\", re.IGNORECASE)\n",
    "regex_user2 = re.compile(\"\\[{1,2}(User:|User Talk:|User talk:|User_talk:)\\s?\\w*(\\||\\]{1,2})?\", re.IGNORECASE)\n",
    "regex_list_user = [regex_user1, regex_user2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_css = re.compile('(style|class|cellpadding|cellspacing|width|colspan|rowspan|valign|align|id)\\s?=\\s?\\\"\\\".*?\\\"\\\"', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_new_line(comment, regex):\n",
    "    try:\n",
    "        comment = regex.sub(\" \", comment)\n",
    "        return comment\n",
    "    except:\n",
    "        print(comment)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_image_multiple_regex(comment, regexList):\n",
    "    for regex in regexList:\n",
    "        comment = regex.sub(\"<image>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_link_multiple_regex(comment, regexList):\n",
    "    for regex in regexList:\n",
    "        comment = regex.sub(\"<link>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_mutiple_white_space(comment, regex):\n",
    "    comment = regex.sub(\" \", comment).strip()\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_user_multiple_regex(comment, regexList):\n",
    "    for regex in regexList:\n",
    "        comment = regex.sub(\"<user>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_css(comment, regex):\n",
    "    comment = regex.sub(\"<css>\", comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_stem(comment):\n",
    "    word_array = comment.split(\" \")\n",
    "    word_array = [stemmer.stem(x) for x in word_array]\n",
    "    result = \" \".join(word_array)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing empty data...\n",
      "Removing new line...\n",
      "Removing white spaces...\n",
      "Removing images...\n",
      "Removing links...\n",
      "Removing users...\n",
      "Removing css...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Removing empty data...\")\n",
    "trainData['comment_text'].fillna(\"<empty>\", inplace=True)\n",
    "print(\"Removing new line...\")\n",
    "trainData[\"comment_text\"] = trainData[\"comment_text\"].apply(lambda x: remove_new_line(x, regex_new_line))\n",
    "print(\"Removing white spaces...\")\n",
    "trainData[\"comment_text\"] = trainData[\"comment_text\"].apply(lambda x: remove_mutiple_white_space(x, regex_multiple_white_space))\n",
    "print(\"Removing images...\")\n",
    "trainData[\"comment_text\"] = trainData[\"comment_text\"].apply(lambda x: replace_image_multiple_regex(x, regex_list_image))\n",
    "print(\"Removing links...\")\n",
    "trainData[\"comment_text\"] = trainData[\"comment_text\"].apply(lambda x: replace_link_multiple_regex(x, regex_list_link))\n",
    "print(\"Removing users...\")\n",
    "trainData[\"comment_text\"] = trainData[\"comment_text\"].apply(lambda x: replace_user_multiple_regex(x, regex_list_user))\n",
    "print(\"Removing css...\")\n",
    "trainData[\"comment_text\"] = trainData[\"comment_text\"].apply(lambda x: replace_css(x, regex_css))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{| style=\"\"background-color:#F5FFFA; padding:0;\"\" cellpadding=\"\"0\"\" style=\"\"border:1px solid #084080; background-color:#F5FFFA; vertical-align:top; color:#000000;\"\"|\n",
      "{| <css> <css> <css>|\n"
     ]
    }
   ],
   "source": [
    "testString = \"{| style=\\\"\\\"background-color:#F5FFFA; padding:0;\\\"\\\" cellpadding=\\\"\\\"0\\\"\\\" style=\\\"\\\"border:1px solid #084080; background-color:#F5FFFA; vertical-align:top; color:#000000;\\\"\\\"|\"\n",
    "print(testString)\n",
    "print(replace_css(testString, regex_css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing empty data...\n",
      "Removing new line...\n",
      "Removing white spaces...\n",
      "Removing images...\n",
      "Removing links...\n",
      "Removing users...\n",
      "Saving csv...\n",
      "Saved...\n"
     ]
    }
   ],
   "source": [
    "print(\"Removing empty data...\")\n",
    "testData['comment_text'].fillna(\"<empty>\", inplace=True)\n",
    "print(\"Removing new line...\")\n",
    "testData[\"comment_text\"] = testData[\"comment_text\"].apply(lambda x: remove_new_line(x, regex_new_line))\n",
    "print(\"Removing white spaces...\")\n",
    "testData[\"comment_text\"] = testData[\"comment_text\"].apply(lambda x: remove_mutiple_white_space(x, regex_multiple_white_space))\n",
    "print(\"Removing images...\")\n",
    "testData[\"comment_text\"] = testData[\"comment_text\"].apply(lambda x: replace_image_multiple_regex(x, regex_list_image))\n",
    "print(\"Removing links...\")\n",
    "testData[\"comment_text\"] = testData[\"comment_text\"].apply(lambda x: replace_link_multiple_regex(x, regex_list_link))\n",
    "print(\"Removing users...\")\n",
    "testData[\"comment_text\"] = testData[\"comment_text\"].apply(lambda x: replace_user_multiple_regex(x, regex_list_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testData[\"stemmed_text\"] = testData[\"comment_text\"].apply(lambda x: apply_stem(x));\n",
    "trainData[\"stemmed_text\"] = trainData[\"comment_text\"].apply(lambda x: apply_stem(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testData.to_csv(\"test_cleaned.csv\", encoding = \"utf-8\", index = False)\n",
    "trainData.to_csv(\"train_cleaned.csv\", encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_comment(comment): # cleaning to be filled up\n",
    "    if math.isnan(comment):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return str(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textstat Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_syllable(comment):\n",
    "    comment_lst = comment.split(\" \")\n",
    "    syllable_count = []\n",
    "    for word in comment_lst:\n",
    "        try:\n",
    "            syllable_count.extend([textstat.syllable_count(word)])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(syllable_count) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.average(syllable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Translator to remove all puncutuations "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* I did not remove all the punctuations at the data cleaning stage because: \n",
    "    - some of the puncutuations may be useful (ie, emoticons / intensifiers like ?!?!?!?) \n",
    "  \n",
    "  hence, these can be captured during feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get the average emotion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_emotion(comment, dict_):\n",
    "    try:\n",
    "        comment = comment.translate(translator)\n",
    "    except:\n",
    "        comment = \"\"\n",
    "    comment_lst = comment.split(\" \")\n",
    "    score_lst = [dict_.get(word.lower()) for word in comment_lst if dict_.get(word.lower()) is not None]\n",
    "    if len(score_lst) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return np.average(score_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_file(comment):\n",
    "    extensions = [\".jpg\", \".png\", \".jpeg\", \".gif\", \"tiff\"]\n",
    "    \n",
    "    for extension in extensions: \n",
    "        \n",
    "        if extension in comment.lower():\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data File Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFile:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.__path = path\n",
    "        df = pd.read_csv(path).replace(np.nan, '', regex=True)\n",
    "        self.__df = df\n",
    "        \n",
    "    def dataCleaning(self):\n",
    "        self.__df.loc[:, \"comment_text\"] = self.__df.loc[:, \"comment_text\"].apply(lambda x : clean_comment(x) )\n",
    "    \n",
    "    def getDF(self):\n",
    "        return self.__df\n",
    "    \n",
    "    def updateDF(self, new_df):\n",
    "        self.__df = new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeaturesEngineer:\n",
    "    \n",
    "    def __init__(self, emotion_lexicon_path):\n",
    "        self.emotion_lexicon_path = emotion_lexicon_path\n",
    "        self.emotion_lexicon = pd.read_csv(self.emotion_lexicon_path, sep = \"\\t\")\n",
    "        \n",
    "        self.anger = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"anger\", :]\n",
    "        self.fear = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"fear\", :]\n",
    "        self.joy = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"joy\", :]\n",
    "        self.sadness = self.emotion_lexicon.loc[self.emotion_lexicon.loc[:, \"AffectDimension\"] == \"sadness\", :]\n",
    "        \n",
    "        anger_dict = {}\n",
    "        for i in range(0, self.anger.shape[0]):\n",
    "            word = self.anger.iloc[i, :].term\n",
    "            score = self.anger.iloc[i, :].score\n",
    "            anger_dict[word] = score\n",
    "            \n",
    "        fear_dict = {}\n",
    "        for i in range(0, self.fear.shape[0]):\n",
    "            word = self.fear.iloc[i, :].term\n",
    "            score = self.fear.iloc[i, :].score\n",
    "            fear_dict[word] = score\n",
    "            \n",
    "        joy_dict = {}\n",
    "        for i in range(0, self.joy.shape[0]):\n",
    "            word = self.joy.iloc[i, :].term\n",
    "            score = self.joy.iloc[i, :].score\n",
    "            joy_dict[word] = score\n",
    "            \n",
    "        sadness_dict = {}\n",
    "        for i in range(0, self.sadness.shape[0]):\n",
    "            word = self.sadness.iloc[i, :].term\n",
    "            score = self.sadness.iloc[i, :].score\n",
    "            sadness_dict[word] = score\n",
    "            \n",
    "        self.anger_dict = anger_dict\n",
    "        self.fear_dict = fear_dict\n",
    "        self.joy_dict = joy_dict\n",
    "        self.sadness_dict = sadness_dict\n",
    "    \n",
    "    def createTextStatFeatures(self, df):\n",
    "        \n",
    "        df.loc[:,'comment_len'] = df.loc[:,'comment_text'].apply(lambda x: len(x))\n",
    "        df.loc[:,'comment_avg_syllable'] = df.loc[:,'comment_text'].apply(lambda x: average_syllable(x))\n",
    "        df.loc[:,'comment_syllable'] = df.loc[:,'comment_text'].apply(lambda x: textstat.syllable_count(x))\n",
    "        df.loc[:,'comment_num_sent'] = df.loc[:, 'comment_text'].apply(lambda x: textstat.sentence_count(x))\n",
    "        df.loc[:, \"comment_word_per_sent\"] = df.loc[:, \"comment_text\"].apply(lambda x: textstat.lexicon_count(x) / textstat.sentence_count(x))\n",
    "        df.loc[:,'comment_flesch_reading_ease'] = df.loc[:,'comment_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "        df.loc[:,'comment_flesch_kincaid_grade'] = df.loc[:,'comment_text'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def createEmotionFeatures(self, df):\n",
    "        \n",
    "        df.loc[:, \"avg_anger\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.anger_dict))\n",
    "        df.loc[:, \"avg_fear\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.fear_dict))\n",
    "        df.loc[:, \"avg_joy\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.joy_dict))\n",
    "        df.loc[:, \"avg_sadness\"] = df.loc[:, \"comment_text\"].apply(lambda x: avg_emotion(x, self.sadness_dict))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def booleanFeatures()\n",
    "    \n",
    "    def intensifiersFeatures(self, df):\n",
    "        \n",
    "        df.loc[:, \"emphasized_words\"] = df.loc[:, \"comment_text\"].apply(lambda x: x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_func(y_label, y_proba):\n",
    "    return log_loss(y_label, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get the engineered features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_manual_features(df):\n",
    "    \n",
    "    col = [\"id\", \"comment_text\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\" ]\n",
    "    df_sieved = df.drop(col, axis = 1)\n",
    "    \n",
    "    return df_sieved\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning to FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_manual_features_ft = FunctionTransformer(get_manual_features, validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do grid search for the best C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data in nr-fold cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(num, cv):\n",
    "    \n",
    "    num_points = int(num/cv)\n",
    "    num_points_last = num - (cv-1) * num_points\n",
    "    selected = []\n",
    "    index_test = []\n",
    "    index_train = []\n",
    "    \n",
    "    for i in range(0, cv):\n",
    "        if i == cv-1:\n",
    "            choices = [x for x in range(0, num) if x not in selected]\n",
    "            picks = random.sample(choices, num_points_last)\n",
    "            remaining = [x for x in range(0, num) if x not in picks]\n",
    "            index_test.append(picks)\n",
    "            index_train.append(remaining)\n",
    "            selected.extend(picks)\n",
    "        else:\n",
    "            choices = [x for x in range(0, num) if x not in selected]\n",
    "            picks = random.sample(choices, num_points)\n",
    "            remaining = [x for x in range(0, num) if x not in picks]\n",
    "            index_test.append(picks)\n",
    "            index_train.append(remaining)\n",
    "            selected.extend(picks)\n",
    "            \n",
    "    return index_train, index_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_C(min_c, max_c, step, data, label, cv):\n",
    "    index_train, index_test = split_data(data.shape[0], cv)\n",
    "    c_range = np.arange(min_c, max_c, step)\n",
    "    loss_lst_compiled = []\n",
    "    for c in c_range:\n",
    "        val = 2.0 ** c\n",
    "        loss_lst = []\n",
    "        for i in range(0, cv):\n",
    "            test_ = index_test[i]\n",
    "            train_ = index_train[i]\n",
    "            \n",
    "            features_train = data.iloc[train_, :]\n",
    "            labels_train = data.iloc[train_, :].loc[:, label]\n",
    "            \n",
    "            features_test = data.iloc[test_, :]\n",
    "            labels_test = data.iloc[test_, :].loc[:, label]\n",
    "            \n",
    "            param = \"-c \" + str(val) + \" -s \" + str(self.algo)\n",
    "    \n",
    "            pipe.fit(features_train, labels_train)\n",
    "            labels = pipe.predict(features_test)\n",
    "            proba = pipe.predict_proba(features_test)\n",
    "            \n",
    "            loss = loss_func(labels_test, proba)\n",
    "            loss_lst.extend([loss])\n",
    "        \n",
    "        avg_loss = np.average(loss_lst)\n",
    "        loss_lst_compiled.append(avg_loss)\n",
    "        \n",
    "    loss_lst_compiled = [x for x in zip(c_range, loss_lst_compiled)]  \n",
    "    loss_lst_compiled = sorted(loss_lst_compiled, key= lambda x: x[1])\n",
    "    return  loss_lst_compiled[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    loss_compiled = []\n",
    "    \n",
    "    for label in labels:\n",
    "        best_c, best_rate = best_C(-5,5,1, trainData.getDF(), label, 5)\n",
    "        print('Log loss for \"{}\" based on 5 fold CV: {}, with log2c = {}'.format(label, best_rate, best_c))\n",
    "        loss_compiled.extend([best_rate])\n",
    "    \n",
    "    print(loss_compiled)\n",
    "    avg_loss = np.average(loss_compiled)\n",
    "    print('Average Log loss = {}'.format(avg_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #extractor = FeaturesEngineer(emotion_lexicon_path)\n",
    "    #trainData = DataFile(path = train_path)\n",
    "    print(\"Done reading in train data\")\n",
    "    #testData = DataFile(path= test_path)\n",
    "    #print(\"Done reading in test data\")\n",
    "    \n",
    "    #trainData.updateDF(extractor.createEmotionFeatures(trainData.getDF()))\n",
    "    print(\"Done creating emotion features for train data\")\n",
    "    #testData.updateDF(extractor.createEmotionFeatures(testData.getDF()))\n",
    "    #print(\"Done creating emotion features for test data\")\n",
    "    \n",
    "    #trainData.updateDF(extractor.createTextStatFeatures(trainData.getDF()))\n",
    "    print(\"Done creating textstat features for train data\")\n",
    "    #testData.updateDF(extractor.createTextStatFeatures(testData.getDF()))\n",
    "    #print(\"Done creating textstat features for test data\")\n",
    "    \n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "200 max iterations: \n",
    "Log loss for \"toxic\" based on 5 fold CV: 0.2888927887844984, with log2c = 0\n",
    "Log loss for \"severe_toxic\" based on 5 fold CV: 0.052191609111000556, with log2c = 3\n",
    "Log loss for \"obscene\" based on 5 fold CV: 0.19248318827050923, with log2c = -1\n",
    "Log loss for \"threat\" based on 5 fold CV: 0.016697640474759634, with log2c = 4\n",
    "Log loss for \"insult\" based on 5 fold CV: 0.1815360776373493, with log2c = 0\n",
    "Log loss for \"identity_hate\" based on 5 fold CV: 0.04793890910190407, with log2c = -1\n",
    "Average Log loss = 0.12995670223000352"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "100 max_iterations: \n",
    "\n",
    "Log loss for \"toxic\" based on 5 fold CV: 0.28947272723407264, with log2c = 2\n",
    "Log loss for \"severe_toxic\" based on 5 fold CV: 0.05202002362824583, with log2c = 2\n",
    "Log loss for \"obscene\" based on 5 fold CV: 0.19150573839222035, with log2c = 2\n",
    "Log loss for \"threat\" based on 5 fold CV: 0.01670371289983063, with log2c = 1\n",
    "Log loss for \"insult\" based on 5 fold CV: 0.1808462941673421, with log2c = 2\n",
    "Log loss for \"identity_hate\" based on 5 fold CV: 0.04778838394220845, with log2c = 1\n",
    "Average Log loss = 0.12972281337731997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData.getDF().to_csv(\"../data/features/train_data_cache.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression(max_iter=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
