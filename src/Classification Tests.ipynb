{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:45.074319Z",
     "start_time": "2017-12-29T11:50:44.252319Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim-3.2.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from NbSvmClassifier import NbSvmClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# BoW feature extraction\n",
    "import re, string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.614315Z",
     "start_time": "2017-12-29T11:50:45.075316Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "sample_submission_file = '../data/sample_submission.csv'\n",
    "\n",
    "train_all = pd.read_csv(train_file)\n",
    "test_for_submission = pd.read_csv(test_file)\n",
    "sample_submission = pd.read_csv(sample_submission_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Account for more cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.645319Z",
     "start_time": "2017-12-29T11:50:46.615318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test_for_submission['comment_text'].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.653316Z",
     "start_time": "2017-12-29T11:50:46.646319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=0)\n",
    "cv_splits_train = []\n",
    "cv_splits_test = []\n",
    "for train_index, test_index in kf.split(train_all):\n",
    "    cv_splits_train.append(train_index)\n",
    "    cv_splits_test.append(test_index)\n",
    "cv_folds = len(cv_splits_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.714315Z",
     "start_time": "2017-12-29T11:50:46.654317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_train = []\n",
    "cv_test = []\n",
    "for i in range(cv_folds):\n",
    "    cv_train.append(train_all.loc[cv_splits_train[i], :])\n",
    "    cv_test.append(train_all.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.718319Z",
     "start_time": "2017-12-29T11:50:46.715318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = cv_train\n",
    "test = cv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and check that the data is split such that the percentages of labels in train/test are roughly equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.725317Z",
     "start_time": "2017-12-29T11:50:46.719318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.818317Z",
     "start_time": "2017-12-29T11:50:46.726318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV train 0\n",
      "        toxic: 7366 / 76680 (9.606%)\n",
      " severe_toxic: 792 / 76680 (1.033%)\n",
      "      obscene: 4108 / 76680 (5.357%)\n",
      "       threat: 234 / 76680 (0.305%)\n",
      "       insult: 3826 / 76680 (4.99%)\n",
      "identity_hate: 654 / 76680 (0.853%)\n",
      "CV test 0\n",
      "        toxic: 1871 / 19171 (9.76%)\n",
      " severe_toxic: 173 / 19171 (0.902%)\n",
      "      obscene: 1001 / 19171 (5.221%)\n",
      "       threat: 71 / 19171 (0.37%)\n",
      "       insult: 939 / 19171 (4.898%)\n",
      "identity_hate: 160 / 19171 (0.835%)\n",
      "CV train 1\n",
      "        toxic: 7413 / 76681 (9.667%)\n",
      " severe_toxic: 772 / 76681 (1.007%)\n",
      "      obscene: 4136 / 76681 (5.394%)\n",
      "       threat: 244 / 76681 (0.318%)\n",
      "       insult: 3841 / 76681 (5.009%)\n",
      "identity_hate: 657 / 76681 (0.857%)\n",
      "CV test 1\n",
      "        toxic: 1824 / 19170 (9.515%)\n",
      " severe_toxic: 193 / 19170 (1.007%)\n",
      "      obscene: 973 / 19170 (5.076%)\n",
      "       threat: 61 / 19170 (0.318%)\n",
      "       insult: 924 / 19170 (4.82%)\n",
      "identity_hate: 157 / 19170 (0.819%)\n",
      "CV train 2\n",
      "        toxic: 7397 / 76681 (9.646%)\n",
      " severe_toxic: 764 / 76681 (0.996%)\n",
      "      obscene: 4072 / 76681 (5.31%)\n",
      "       threat: 248 / 76681 (0.323%)\n",
      "       insult: 3795 / 76681 (4.949%)\n",
      "identity_hate: 651 / 76681 (0.849%)\n",
      "CV test 2\n",
      "        toxic: 1840 / 19170 (9.598%)\n",
      " severe_toxic: 201 / 19170 (1.049%)\n",
      "      obscene: 1037 / 19170 (5.409%)\n",
      "       threat: 57 / 19170 (0.297%)\n",
      "       insult: 970 / 19170 (5.06%)\n",
      "identity_hate: 163 / 19170 (0.85%)\n",
      "CV train 3\n",
      "        toxic: 7392 / 76681 (9.64%)\n",
      " severe_toxic: 769 / 76681 (1.003%)\n",
      "      obscene: 4068 / 76681 (5.305%)\n",
      "       threat: 254 / 76681 (0.331%)\n",
      "       insult: 3818 / 76681 (4.979%)\n",
      "identity_hate: 652 / 76681 (0.85%)\n",
      "CV test 3\n",
      "        toxic: 1845 / 19170 (9.624%)\n",
      " severe_toxic: 196 / 19170 (1.022%)\n",
      "      obscene: 1041 / 19170 (5.43%)\n",
      "       threat: 51 / 19170 (0.266%)\n",
      "       insult: 947 / 19170 (4.94%)\n",
      "identity_hate: 162 / 19170 (0.845%)\n",
      "CV train 4\n",
      "        toxic: 7380 / 76681 (9.624%)\n",
      " severe_toxic: 763 / 76681 (0.995%)\n",
      "      obscene: 4052 / 76681 (5.284%)\n",
      "       threat: 240 / 76681 (0.313%)\n",
      "       insult: 3780 / 76681 (4.93%)\n",
      "identity_hate: 642 / 76681 (0.837%)\n",
      "CV test 4\n",
      "        toxic: 1857 / 19170 (9.687%)\n",
      " severe_toxic: 202 / 19170 (1.054%)\n",
      "      obscene: 1057 / 19170 (5.514%)\n",
      "       threat: 65 / 19170 (0.339%)\n",
      "       insult: 985 / 19170 (5.138%)\n",
      "identity_hate: 172 / 19170 (0.897%)\n"
     ]
    }
   ],
   "source": [
    "def print_count_of_each_label(df):\n",
    "    for label in labels:\n",
    "        print('{}: {} / {} ({}%)'.format(label.rjust(len(labels[-1])),\n",
    "                                         df.loc[df[label] == 1].shape[0],\n",
    "                                         len(df),\n",
    "                                         np.round(df.loc[df[label] == 1].shape[0]/len(df)*100, 3)))\n",
    "\n",
    "for i in range(cv_folds):\n",
    "    print('CV train {}'.format(i))\n",
    "    print_count_of_each_label(cv_train[i])\n",
    "    print('CV test {}'.format(i))\n",
    "    print_count_of_each_label(cv_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test into X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we can use sklearn classifiers easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:47.241326Z",
     "start_time": "2017-12-29T11:50:47.212328Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [None] * cv_folds\n",
    "y_train = [None] * cv_folds\n",
    "X_test = [None] * cv_folds\n",
    "y_test = [None] * cv_folds\n",
    "for i in range(cv_folds):\n",
    "    X_train[i], y_train[i] = train[i][[\"comment_text\"]], train[i][labels]\n",
    "    X_test[i], y_test[i] = test[i][[\"comment_text\"]], test[i][labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:47.456327Z",
     "start_time": "2017-12-29T11:50:47.452327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 1) (76680, 6)\n",
      "(19171, 1) (19171, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape, y_train[0].shape)\n",
    "print(X_test[0].shape, y_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:48.253489Z",
     "start_time": "2017-12-29T11:50:48.243491Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_textstat_features(df):\n",
    "    features_df = pd.DataFrame()\n",
    "    features_df['comment_text_len'] = df['comment_text'].apply(len)\n",
    "    features_df['comment_text_lex_count'] = df['comment_text'].apply(textstat.lexicon_count)\n",
    "    features_df['comment_text_syl_count'] = df['comment_text'].apply(textstat.syllable_count)\n",
    "    features_df['comment_text_sent_count'] = df['comment_text'].apply(textstat.sentence_count)\n",
    "    features_df['comment_text_flesch_reading_ease'] = df['comment_text'].apply(textstat.flesch_reading_ease)\n",
    "    features_df['comment_text_flesch_kincaid_grade'] = df['comment_text'].apply(textstat.flesch_kincaid_grade)\n",
    "    \n",
    "    features_df['comment_text_syl_over_lex'] = features_df['comment_text_syl_count'] / features_df['comment_text_lex_count']\n",
    "    features_df['comment_text_lex_over_sent'] = features_df['comment_text_lex_count'] / features_df['comment_text_sent_count']\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:48.543487Z",
     "start_time": "2017-12-29T11:50:48.459489Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_textstat_features_file = '../data/train_textstat_features.csv'\n",
    "if os.path.isfile(train_textstat_features_file):\n",
    "    X_train_all_features_textstat = pd.read_csv(train_textstat_features_file, index_col=0)\n",
    "else:\n",
    "    X_train_all_features_textstat = extract_textstat_features(train_all)\n",
    "    X_train_all_features_textstat.to_csv(train_textstat_features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:21:02.663558Z",
     "start_time": "2017-12-25T03:21:02.123907Z"
    }
   },
   "source": [
    "## Split textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:49.183579Z",
     "start_time": "2017-12-29T11:50:49.139581Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_textstat = []\n",
    "X_test_features_textstat = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_train[i], :])\n",
    "    X_test_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:49.656626Z",
     "start_time": "2017-12-29T11:50:49.651627Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_tok = re.compile('([{}“”¨«»®´·º½¾¿¡§£₤‘’])'.format(string.punctuation))\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:15.383547Z",
     "start_time": "2017-12-29T11:50:55.404544Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = train_all.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "train_term_doc = vec.fit_transform(train_all['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.182547Z",
     "start_time": "2017-12-29T11:51:15.384547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_bow = []\n",
    "X_test_features_bow = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_bow.append(train_term_doc[cv_splits_train[i], :])\n",
    "    X_test_features_bow.append(train_term_doc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract emotion scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.189550Z",
     "start_time": "2017-12-29T11:51:16.183550Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_lexicon_file = \"../data/features/NRC-AffectIntensity-Lexicon.txt\"\n",
    "emotion_lexicon = pd.read_csv(emotion_lexicon_file, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.198573Z",
     "start_time": "2017-12-29T11:51:16.190549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_term_score = {'anger': {}, 'fear': {}, 'joy': {}, 'sadness': {}}\n",
    "for row in emotion_lexicon.itertuples():\n",
    "    emotion_term_score[row.AffectDimension][row.term] = row.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.212546Z",
     "start_time": "2017-12-29T11:51:16.199547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness']\n",
    "def avg_emotion_score(comment_text):\n",
    "    try:\n",
    "        comment_cleaned = comment_text.translate(translator)\n",
    "    except:\n",
    "        comment_cleaned = \"\"\n",
    "    comment_cleaned = comment_cleaned.lower()\n",
    "    comment_cleaned_words = comment_cleaned.split(\" \")\n",
    "    \n",
    "    emotion_scores = {'anger': 0, 'fear': 0, 'joy': 0, 'sadness': 0}\n",
    "    for emotion in emotions:\n",
    "        scores = [emotion_term_score[emotion].get(word) for word in comment_cleaned_words\n",
    "                    if emotion_term_score[emotion].get(word) is not None]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        emotion_scores[emotion] = np.mean(scores)\n",
    "        \n",
    "    return [emotion_scores[emotion] for emotion in emotions]\n",
    "    \n",
    "def extract_emotion_features(df):\n",
    "    features_df = df['comment_text'].apply(avg_emotion_score)\n",
    "    return pd.DataFrame(features_df.values.tolist(), columns=['comment_text_emotion_{}'.format(emotion) for emotion in emotions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:21.960189Z",
     "start_time": "2017-12-29T11:51:16.213549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_all_features_emotion = extract_emotion_features(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:22.001189Z",
     "start_time": "2017-12-29T11:51:21.961192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_emotion = []\n",
    "X_test_features_emotion = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_emotion.append(X_train_all_features_emotion.loc[cv_splits_train[i], :])\n",
    "    X_test_features_emotion.append(X_train_all_features_emotion.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:01:28.405401Z",
     "start_time": "2017-12-29T11:01:28.402400Z"
    }
   },
   "source": [
    "## Extract Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:50.085194Z",
     "start_time": "2017-12-29T11:51:22.002192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:50.091195Z",
     "start_time": "2017-12-29T11:51:50.086197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_w2v(comment_text):\n",
    "    sentence = tokenize(comment_text)\n",
    "    f = np.zeros(w2v_model.vector_size)\n",
    "    for w in sentence:\n",
    "        try:\n",
    "            vec = w2v_model[w]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        f += vec\n",
    "    f /= len(sentence)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:13.850198Z",
     "start_time": "2017-12-29T11:51:50.092195Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v = train_all['comment_text'].apply(featurize_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:13.977199Z",
     "start_time": "2017-12-29T11:52:13.851200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v = np.array(X_train_all_features_w2v.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.271199Z",
     "start_time": "2017-12-29T11:52:13.978200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_features_w2v = []\n",
    "X_test_features_w2v = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_w2v.append(X_train_all_features_w2v[cv_splits_train[i]])\n",
    "    X_test_features_w2v.append(X_train_all_features_w2v[cv_splits_test[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to calculate mean column-wise log loss of y_pred vs y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.275203Z",
     "start_time": "2017-12-29T11:52:14.272199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_actual, y_pred):\n",
    "    col_log_loss = [log_loss(np.array(y_actual[label]),\n",
    "                             np.array([1.-np.array(y_pred[label]), np.array(y_pred[label])]).T) for label in labels]\n",
    "    return col_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classification scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.301199Z",
     "start_time": "2017-12-29T11:52:14.276203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9920072216264108e-16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(calculate_score(y_test[0], y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.397202Z",
     "start_time": "2017-12-29T11:52:14.302201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.32844347  0.34772673  1.84096823  0.10990296  1.71701177  0.29331534]\n",
      "1.27289475201\n"
     ]
    }
   ],
   "source": [
    "scores_zeror = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.zeros(len(labels))] * len(X_test[i]))\n",
    "    y_pred_zeror = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_zeror)\n",
    "    scores_zeror.append(scores)\n",
    "print(np.mean(scores_zeror, axis=0))\n",
    "print(np.mean(scores_zeror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-24T12:28:47.459632Z",
     "start_time": "2017-12-24T12:28:47.455633Z"
    }
   },
   "source": [
    "## All 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.490201Z",
     "start_time": "2017-12-29T11:52:14.398201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69314718  0.69314718  0.69314718  0.69314718  0.69314718  0.69314718]\n",
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "scores_half = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.ones(len(labels))*0.5] * len(X_test[i]))\n",
    "    y_pred_half = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_half)\n",
    "    scores_half.append(scores)\n",
    "print(np.mean(scores_half, axis=0))\n",
    "print(np.mean(scores_half))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test calculate_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.588202Z",
     "start_time": "2017-12-29T11:52:14.491203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.69314718   0.91220862   1.15881057   1.6050267    2.19335539\n",
      "  34.24546105]\n",
      "6.80133491769\n"
     ]
    }
   ],
   "source": [
    "scores_half = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])] * len(X_test[i]))\n",
    "    y_pred_half = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_half)\n",
    "    scores_half.append(scores)\n",
    "print(np.mean(scores_half, axis=0))\n",
    "print(np.mean(scores_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.594202Z",
     "start_time": "2017-12-29T11:52:14.589200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_classifier_per_label(classifiers, y_pred, scores):\n",
    "    classifier_scores = np.array([np.mean(scores[classifier_name], axis=0) for classifier_name, _ in classifiers])\n",
    "    best_classifier_score_per_label = [(classifiers[min_idx][0], classifier_scores[min_idx][i])\n",
    "                                           for i, min_idx in enumerate(np.argmin(classifier_scores, axis=0))]\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\"[{}] {} : {}\".format(label, best_classifier_score_per_label[i][0], best_classifier_score_per_label[i][1]))\n",
    "    print(\"Average: {}\".format(np.mean([x[1] for x in best_classifier_score_per_label])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.603200Z",
     "start_time": "2017-12-29T11:52:14.595201Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classifiers(classifiers, X_train_features, X_test_features, y_train, clf, y_pred, scores):\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        print('Training with {}'.format(classifier_name))\n",
    "        clf[classifier_name] = [{}]*cv_folds\n",
    "        y_pred[classifier_name] = [{}]*cv_folds\n",
    "        scores[classifier_name] = [{}]*cv_folds\n",
    "        for fold in range(cv_folds):\n",
    "            y_pred[classifier_name][fold] = pd.DataFrame()\n",
    "            for label in labels:\n",
    "                clf[classifier_name][fold][label] = classifier()\n",
    "                clf[classifier_name][fold][label].fit(X_train_features[fold], y_train[fold][label])\n",
    "\n",
    "                y_pred[classifier_name][fold][label] = clf[classifier_name][fold][label].predict_proba(X_test_features[fold]).T[1]\n",
    "\n",
    "            scores[classifier_name][fold] = calculate_score(y_test[fold], y_pred[classifier_name][fold])\n",
    "        print('Column-wise log loss for {}: {} - {}'.format(classifier_name, np.mean(scores[classifier_name], axis=0), np.mean(scores[classifier_name])))\n",
    "    best_classifier_per_label(classifiers, y_pred, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:26:24.881527Z",
     "start_time": "2017-12-29T12:26:24.871025Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_textstat = []\n",
    "# classifiers_textstat += [\n",
    "#     ('Logistic Regression {}'.format(c), lambda: LogisticRegression(solver=\"newton-cg\", C=2.0**c, max_iter=1000)) for c in np.arange(-3,4,1)]\n",
    "classifiers_textstat += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "    ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "    (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "    (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "    (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "    (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "    (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "    (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "    (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "    (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "    # Cannot even run\n",
    "    #     (\"Gaussian Process\", lambda: GaussianProcessClassifier(1.0 * RBF(1.0)))  # Memory error??? Even with 32GB ram???\n",
    "    #     (\"Linear SVM\", lambda: SVC(kernel=\"linear\", C=0.025)),                   # Slow like shit\n",
    "    #     (\"RBF SVM\", lambda: SVC(gamma=2, C=1))                                   # Slow like shit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:32:36.401372Z",
     "start_time": "2017-12-29T12:26:25.061817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.28483668  0.04888504  0.18583719  0.01931983  0.17817057  0.04614669] - 0.12719933169122624\n",
      "Training with Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:414: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column-wise log loss for Logistic Regression: [ 0.30794287  0.0546909   0.20307196  0.02067343  0.19241465  0.0487866 ] - 0.1379300674824823\n",
      "Training with NB-SVM\n",
      "Column-wise log loss for NB-SVM: [ 0.3088186   0.05477122  0.2064551   0.02241579  0.19257606  0.04878047] - 0.13896953998377357\n",
      "Training with Decision Tree\n",
      "Column-wise log loss for Decision Tree: [ 0.29733778  0.05873745  0.1937015   0.02252335  0.18638307  0.05241342] - 0.1351827609818167\n",
      "Training with Random Forest\n",
      "Column-wise log loss for Random Forest: [ 0.28832661  0.0497639   0.18785226  0.01962468  0.18033365  0.04646596] - 0.1287278421113957\n",
      "Training with Extra Trees\n",
      "Column-wise log loss for Extra Trees: [ 0.30738822  0.05342604  0.20249935  0.02103595  0.19127086  0.0483372 ] - 0.1373262719020896\n",
      "Training with Gradient Boosting\n",
      "Column-wise log loss for Gradient Boosting: [ 0.28496404  0.0507307   0.18617221  0.02223036  0.17856467  0.04868675] - 0.12855812259564575\n",
      "Training with Nearest Neighbors\n",
      "Column-wise log loss for Nearest Neighbors: [ 0.30040041  0.09996997  0.22047394  0.05986065  0.2100507   0.11631126] - 0.16784448759227708\n",
      "Training with Naive Bayes\n",
      "Column-wise log loss for Naive Bayes: [ 1.19991102  0.6043399   0.66073099  0.39049619  0.55816932  0.41003738] - 0.6372808012714472\n",
      "Training with Neural Net\n",
      "Column-wise log loss for Neural Net: [ 0.3742152   0.08502062  0.25995982  0.03191875  0.21683193  0.0719353 ] - 0.17331360402766957\n",
      "Training with AdaBoost\n",
      "Column-wise log loss for AdaBoost: [ 0.67160435  0.63981316  0.66305688  0.62246185  0.66246049  0.64227525] - 0.6502786638692973\n",
      "Training with QDA\n",
      "Column-wise log loss for QDA: [ 1.02215137  0.26864289  0.5120429   0.108893    0.45853827  0.19476892] - 0.42750622620456047\n",
      "[toxic] XGBoost : 0.284836675541959\n",
      "[severe_toxic] XGBoost : 0.04888503970659461\n",
      "[obscene] XGBoost : 0.18583719111278976\n",
      "[threat] XGBoost : 0.0193198264879471\n",
      "[insult] XGBoost : 0.17817056803190126\n",
      "[identity_hate] XGBoost : 0.04614668926616587\n",
      "Average: 0.12719933169122624\n"
     ]
    }
   ],
   "source": [
    "clf_textstat = {}\n",
    "y_pred_textstat = {}\n",
    "scores_textstat = {}\n",
    "run_classifiers(classifiers_textstat, X_train_features_textstat, X_test_features_textstat, y_train,\n",
    "                clf_textstat, y_pred_textstat, scores_textstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:32:36.410873Z",
     "start_time": "2017-12-29T12:32:36.402372Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_emotion = []\n",
    "classifiers_emotion += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "    ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "    (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "    (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "    (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "    (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "    (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "    (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "    (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "    (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:37:57.589171Z",
     "start_time": "2017-12-29T12:32:36.411873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.28097739  0.0491046   0.18069825  0.01608059  0.17323072  0.04627828] - 0.12439497224533132\n",
      "Training with Logistic Regression\n",
      "Column-wise log loss for Logistic Regression: [ 0.30281782  0.05346437  0.19816069  0.01778966  0.18850244  0.04790268] - 0.13477294240101634\n",
      "Training with NB-SVM\n",
      "Column-wise log loss for NB-SVM: [ 0.30284635  0.05354436  0.19828223  0.0178291   0.18855963  0.04798816] - 0.13484163988018344\n",
      "Training with Decision Tree\n",
      "Column-wise log loss for Decision Tree: [ 0.29117012  0.05792187  0.1897252   0.02282945  0.17987671  0.05247347] - 0.13233280379148019\n",
      "Training with Random Forest\n",
      "Column-wise log loss for Random Forest: [ 0.2888855   0.05026322  0.1867915   0.0167407   0.17877991  0.04701277] - 0.12807893075084542\n",
      "Training with Extra Trees\n",
      "Column-wise log loss for Extra Trees: [ 0.29613054  0.05185875  0.19378921  0.01694965  0.18363259  0.04722122] - 0.1315969921317449\n",
      "Training with Gradient Boosting\n",
      "Column-wise log loss for Gradient Boosting: [ 0.28032099  0.05082728  0.1799619   0.02550301  0.17277558  0.04804537] - 0.1262390202797254\n",
      "Training with Nearest Neighbors\n",
      "Column-wise log loss for Nearest Neighbors: [ 0.29165544  0.15037621  0.20590929  0.04238035  0.19490669  0.15876168] - 0.17399827741015292\n",
      "Training with Naive Bayes\n",
      "Column-wise log loss for Naive Bayes: [ 0.34974573  0.06386406  0.22037707  0.05350175  0.21354214  0.05394995] - 0.1591634489533457\n",
      "Training with Neural Net\n",
      "Column-wise log loss for Neural Net: [ 0.30556634  0.05603648  0.20252119  0.02144513  0.19254445  0.04888784] - 0.13783357137330873\n",
      "Training with AdaBoost\n",
      "Column-wise log loss for AdaBoost: [ 0.67279384  0.6438545   0.66484652  0.62444232  0.66409402  0.64428087] - 0.6523853452278163\n",
      "Training with QDA\n",
      "Column-wise log loss for QDA: [ 0.32082022  0.05767759  0.20768918  0.02257206  0.19890271  0.04920142] - 0.1428105301310837\n",
      "[toxic] Gradient Boosting : 0.28032098830370933\n",
      "[severe_toxic] XGBoost : 0.04910460056419642\n",
      "[obscene] Gradient Boosting : 0.179961899993122\n",
      "[threat] XGBoost : 0.01608059269426635\n",
      "[insult] Gradient Boosting : 0.172775583787855\n",
      "[identity_hate] XGBoost : 0.04627828065343444\n",
      "Average: 0.12408699099943056\n"
     ]
    }
   ],
   "source": [
    "clf_emotion = {}\n",
    "y_pred_emotion = {}\n",
    "scores_emotion = {}\n",
    "run_classifiers(classifiers_emotion, X_train_features_emotion, X_test_features_emotion, y_train,\n",
    "                clf_emotion, y_pred_emotion, scores_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat + emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:37:57.600671Z",
     "start_time": "2017-12-29T12:37:57.590171Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_textstat_emotion = []\n",
    "classifiers_textstat_emotion += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "    ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "    (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "    (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "    (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "    (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "    (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "    (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "    (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "    (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:37:57.635671Z",
     "start_time": "2017-12-29T12:37:57.601671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 8)\n",
      "(76680, 4)\n",
      "(76680, 12)\n"
     ]
    }
   ],
   "source": [
    "# concat features\n",
    "X_train_features_textstat_emotion = [None]*cv_folds\n",
    "X_test_features_textstat_emotion = [None]*cv_folds\n",
    "for fold in range(cv_folds):\n",
    "    X_train_features_textstat_emotion[fold] = np.hstack((X_train_features_textstat[fold], X_train_features_emotion[fold].values))\n",
    "    X_test_features_textstat_emotion[fold] = np.hstack((X_test_features_textstat[fold], X_test_features_emotion[fold].values))\n",
    "\n",
    "print(X_train_features_textstat[0].shape)\n",
    "print(X_train_features_emotion[0].shape)\n",
    "print(X_train_features_textstat_emotion[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:46:31.586121Z",
     "start_time": "2017-12-29T12:37:57.636671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.25466106  0.0439385   0.16512503  0.01459849  0.15957165  0.0441224 ] - 0.11366952025039936\n",
      "Training with Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:414: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column-wise log loss for Logistic Regression: [ 0.29033674  0.05177318  0.19207383  0.01697265  0.18157817  0.04752037] - 0.13004249126162268\n",
      "Training with NB-SVM\n",
      "Column-wise log loss for NB-SVM: [ 0.29464626  0.05191786  0.19800408  0.02284097  0.18207067  0.04755327] - 0.13283885191981842\n",
      "Training with Decision Tree\n",
      "Column-wise log loss for Decision Tree: [ 0.27283879  0.05531407  0.17735648  0.02107284  0.17211927  0.0515602 ] - 0.12504360860616004\n",
      "Training with Random Forest\n",
      "Column-wise log loss for Random Forest: [ 0.27324526  0.04771023  0.17861041  0.01652649  0.17069538  0.04559347] - 0.12206354073120226\n",
      "Training with Extra Trees\n",
      "Column-wise log loss for Extra Trees: [ 0.29108212  0.05083296  0.19218357  0.01747661  0.18325374  0.04715364] - 0.13033044004301256\n",
      "Training with Gradient Boosting\n",
      "Column-wise log loss for Gradient Boosting: [ 0.25477637  0.04651561  0.16593685  0.02140901  0.16028123  0.0469236 ] - 0.11597377738257\n",
      "Training with Nearest Neighbors\n",
      "Column-wise log loss for Nearest Neighbors: [ 0.30000046  0.10052786  0.2199507   0.06007279  0.20983146  0.11570494] - 0.167681367482825\n",
      "Training with Naive Bayes\n",
      "Column-wise log loss for Naive Bayes: [ 1.16824854  0.60374502  0.65205034  0.41148655  0.55087263  0.41325294] - 0.6332760028423154\n",
      "Training with Neural Net\n",
      "Column-wise log loss for Neural Net: [ 0.29762195  0.09218542  0.22838471  0.03054718  0.23998268  0.08931778] - 0.1630066181440449\n",
      "Training with AdaBoost\n",
      "Column-wise log loss for AdaBoost: [ 0.66877762  0.63555532  0.66040046  0.60959585  0.65961676  0.63974013] - 0.6456143569886396\n",
      "Training with QDA\n",
      "Column-wise log loss for QDA: [ 0.97156807  0.264482    0.48611843  0.10188697  0.43447956  0.19135727] - 0.40831538379115545\n",
      "[toxic] XGBoost : 0.254661058308309\n",
      "[severe_toxic] XGBoost : 0.04393850004359558\n",
      "[obscene] XGBoost : 0.16512502878785826\n",
      "[threat] XGBoost : 0.014598489438839646\n",
      "[insult] XGBoost : 0.159571645350197\n",
      "[identity_hate] XGBoost : 0.044122399573596705\n",
      "Average: 0.11366952025039938\n"
     ]
    }
   ],
   "source": [
    "clf_textstat_emotion = {}\n",
    "y_pred_textstat_emotion = {}\n",
    "scores_textstat_emotion = {}\n",
    "run_classifiers(classifiers_textstat_emotion, X_train_features_textstat_emotion,\n",
    "                X_test_features_textstat_emotion, y_train,\n",
    "                clf_textstat_emotion, y_pred_textstat_emotion, scores_textstat_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:46:31.591121Z",
     "start_time": "2017-12-29T12:46:31.587120Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_bow = [\n",
    "    (\"Multinominal Naive Bayes\", lambda: MultinomialNB()),\n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T13:44:57.238874Z",
     "start_time": "2017-12-29T12:46:31.592122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Multinominal Naive Bayes\n",
      "Column-wise log loss for Multinominal Naive Bayes: [ 0.38178366  0.09346046  0.27977229  0.04259091  0.28126247  0.09527841] - 0.19569136478192795\n",
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.17110721  0.02975309  0.0826288   0.01149763  0.09918739  0.02871334] - 0.07048124309883647\n",
      "Training with NB-SVM\n",
      "Column-wise log loss for NB-SVM: [ 0.10984278  0.02989426  0.06178523  0.01158265  0.07788519  0.0280709 ] - 0.053176833214058444\n",
      "Training with Logistic Regression\n",
      "Column-wise log loss for Logistic Regression: [ 0.12359399  0.02828408  0.07265129  0.01201408  0.08213134  0.02879822] - 0.05791216907553058\n",
      "[toxic] NB-SVM : 0.10984278055513888\n",
      "[severe_toxic] Logistic Regression : 0.028284083949432957\n",
      "[obscene] NB-SVM : 0.06178522862661414\n",
      "[threat] XGBoost : 0.011497631662585432\n",
      "[insult] NB-SVM : 0.07788518537680542\n",
      "[identity_hate] NB-SVM : 0.028070896212669443\n",
      "Average: 0.05289430106387438\n"
     ]
    }
   ],
   "source": [
    "clf_bow = {}\n",
    "y_pred_bow = {}\n",
    "scores_bow = {}\n",
    "run_classifiers(classifiers_bow, X_train_features_bow,\n",
    "                X_test_features_bow, y_train,\n",
    "                clf_bow, y_pred_bow, scores_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:15:41.929400Z",
     "start_time": "2017-12-29T12:15:41.925900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_w2v = [\n",
    "    (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "    (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:18:36.896419Z",
     "start_time": "2017-12-29T12:15:43.118302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Random Forest\n",
      "Column-wise log loss for Random Forest: [ 0.26095864  0.04228687  0.16740486  0.01842564  0.15764403  0.04248773] - 0.1148679622405156\n",
      "Training with Extra Trees\n",
      "Column-wise log loss for Extra Trees: [ 0.2545771   0.03963212  0.16112278  0.01777193  0.1532527   0.04061723] - 0.11116230999035771\n",
      "Training with Logistic Regression\n",
      "Column-wise log loss for Logistic Regression: [ 0.1495776   0.03237393  0.10044754  0.01371428  0.10646988  0.03337939] - 0.07266043582520464\n",
      "[toxic] Logistic Regression : 0.14957760244765905\n",
      "[severe_toxic] Logistic Regression : 0.032373928675010925\n",
      "[obscene] Logistic Regression : 0.10044753728050695\n",
      "[threat] Logistic Regression : 0.013714278381529976\n",
      "[insult] Logistic Regression : 0.10646988202741728\n",
      "[identity_hate] Logistic Regression : 0.033379386139103705\n",
      "Average: 0.07266043582520466\n"
     ]
    }
   ],
   "source": [
    "clf_w2v = {}\n",
    "y_pred_w2v = {}\n",
    "scores_w2v = {}\n",
    "\n",
    "run_classifiers(classifiers_w2v, X_train_features_w2v,\n",
    "                X_test_features_w2v, y_train,\n",
    "                clf_w2v, y_pred_w2v, scores_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:20:20.481155Z",
     "start_time": "2017-12-29T12:20:20.478154Z"
    }
   },
   "source": [
    "# Average results from multiple features and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:12:59.934863Z",
     "start_time": "2017-12-29T16:12:59.929864Z"
    }
   },
   "outputs": [],
   "source": [
    "def average_predictions(y_preds, classifier_names, weights):\n",
    "    normalized_weights = np.array(weights) / np.sum(weights)\n",
    "    y_pred_avg = [None] * cv_folds\n",
    "    \n",
    "    for i, fold in enumerate(range(cv_folds)):\n",
    "        for i, y_pred in enumerate(y_preds):\n",
    "            if i==0:\n",
    "                y_pred_avg[fold] = y_pred[classifier_names[i]][fold]*weights[i]\n",
    "            else:\n",
    "                y_pred_avg[fold] += y_pred[classifier_names[i]][fold]*weights[i]\n",
    "    return calculate_score(y_test[fold], y_pred_avg[fold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat+emotion + BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:13:00.588929Z",
     "start_time": "2017-12-29T16:13:00.561933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15394616382376511, 0.031513427585942964, 0.091573173506127772, 0.012349771542211955, 0.10103394365013561, 0.033040015116034986]\n",
      "0.0705760825374\n"
     ]
    }
   ],
   "source": [
    "textstat_emotion_AVG_bow_score = average_predictions([y_pred_textstat_emotion, y_pred_bow],\n",
    "                                                     [\"XGBoost\", \"NB-SVM\"],\n",
    "                                                     [0.5, 0.5])\n",
    "print(textstat_emotion_AVG_bow_score)\n",
    "print(np.mean(textstat_emotion_AVG_bow_score))\n",
    "\n",
    "\n",
    "# 0.11366952025039936\n",
    "# 0.053176833214058444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat+emotion + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:13:04.388452Z",
     "start_time": "2017-12-29T16:13:04.361455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17372593715032622, 0.03506218699394982, 0.11688112899847954, 0.013850713251082796, 0.11884206746101504, 0.036625772074898942]\n",
      "0.082497967655\n"
     ]
    }
   ],
   "source": [
    "textstat_emotion_AVG_w2v_score = average_predictions([y_pred_textstat_emotion, y_pred_w2v],\n",
    "                                                     [\"XGBoost\", \"Logistic Regression\"],\n",
    "                                                     [0.5, 0.5])\n",
    "print(textstat_emotion_AVG_w2v_score)\n",
    "print(np.mean(textstat_emotion_AVG_w2v_score))\n",
    "\n",
    "\n",
    "# 0.11366952025039936\n",
    "# 0.07266043582520464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:18:06.341376Z",
     "start_time": "2017-12-29T16:18:06.313376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10565881024793591, 0.027511218379418043, 0.061340182182013661, 0.01140813046235639, 0.07760912263914016, 0.028494347370178908]\n",
      "0.0520036352135\n"
     ]
    }
   ],
   "source": [
    "bow_AVG_w2v_score = average_predictions([y_pred_bow, y_pred_w2v],\n",
    "                                        [\"NB-SVM\", \"Logistic Regression\"],\n",
    "                                        [0.9, 0.1])\n",
    "print(bow_AVG_w2v_score)\n",
    "print(np.mean(bow_AVG_w2v_score))\n",
    "\n",
    "# 0.053176833214058444\n",
    "# 0.07266043582520464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW + Word2Vec + textstat+emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:19:23.232456Z",
     "start_time": "2017-12-29T16:19:23.201455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10567728340087673, 0.027506816741767196, 0.062615291696673556, 0.01140427014979881, 0.077634489660896797, 0.028496686989087514]\n",
      "0.0522224731065\n"
     ]
    }
   ],
   "source": [
    "bow_AVG_w2v_AVG_textstat_emotion_score = average_predictions([y_pred_bow, y_pred_w2v, y_pred_textstat_emotion],\n",
    "                                        [\"NB-SVM\", \"Logistic Regression\", \"XGBoost\"],\n",
    "                                        [0.9, 0.1, 0.001])\n",
    "print(bow_AVG_w2v_AVG_textstat_emotion_score)\n",
    "print(np.mean(bow_AVG_w2v_AVG_textstat_emotion_score))\n",
    "\n",
    "# 0.053176833214058444\n",
    "# 0.07266043582520464\n",
    "# 0.11366952025039936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
