{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:45.074319Z",
     "start_time": "2017-12-29T11:50:44.252319Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\gensim-3.2.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from NbSvmClassifier import NbSvmClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# BoW feature extraction\n",
    "import re, string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.614315Z",
     "start_time": "2017-12-29T11:50:45.075316Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "sample_submission_file = '../data/sample_submission.csv'\n",
    "\n",
    "train_all = pd.read_csv(train_file)\n",
    "test_for_submission = pd.read_csv(test_file)\n",
    "sample_submission = pd.read_csv(sample_submission_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Account for more cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.645319Z",
     "start_time": "2017-12-29T11:50:46.615318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_all['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test_for_submission['comment_text'].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.653316Z",
     "start_time": "2017-12-29T11:50:46.646319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=0)\n",
    "cv_splits_train = []\n",
    "cv_splits_test = []\n",
    "for train_index, test_index in kf.split(train_all):\n",
    "    cv_splits_train.append(train_index)\n",
    "    cv_splits_test.append(test_index)\n",
    "cv_folds = len(cv_splits_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.714315Z",
     "start_time": "2017-12-29T11:50:46.654317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_train = []\n",
    "cv_test = []\n",
    "for i in range(cv_folds):\n",
    "    cv_train.append(train_all.loc[cv_splits_train[i], :])\n",
    "    cv_test.append(train_all.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.718319Z",
     "start_time": "2017-12-29T11:50:46.715318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = cv_train\n",
    "test = cv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and check that the data is split such that the percentages of labels in train/test are roughly equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.725317Z",
     "start_time": "2017-12-29T11:50:46.719318Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:46.818317Z",
     "start_time": "2017-12-29T11:50:46.726318Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV train 0\n",
      "        toxic: 7366 / 76680 (9.606%)\n",
      " severe_toxic: 792 / 76680 (1.033%)\n",
      "      obscene: 4108 / 76680 (5.357%)\n",
      "       threat: 234 / 76680 (0.305%)\n",
      "       insult: 3826 / 76680 (4.99%)\n",
      "identity_hate: 654 / 76680 (0.853%)\n",
      "CV test 0\n",
      "        toxic: 1871 / 19171 (9.76%)\n",
      " severe_toxic: 173 / 19171 (0.902%)\n",
      "      obscene: 1001 / 19171 (5.221%)\n",
      "       threat: 71 / 19171 (0.37%)\n",
      "       insult: 939 / 19171 (4.898%)\n",
      "identity_hate: 160 / 19171 (0.835%)\n",
      "CV train 1\n",
      "        toxic: 7413 / 76681 (9.667%)\n",
      " severe_toxic: 772 / 76681 (1.007%)\n",
      "      obscene: 4136 / 76681 (5.394%)\n",
      "       threat: 244 / 76681 (0.318%)\n",
      "       insult: 3841 / 76681 (5.009%)\n",
      "identity_hate: 657 / 76681 (0.857%)\n",
      "CV test 1\n",
      "        toxic: 1824 / 19170 (9.515%)\n",
      " severe_toxic: 193 / 19170 (1.007%)\n",
      "      obscene: 973 / 19170 (5.076%)\n",
      "       threat: 61 / 19170 (0.318%)\n",
      "       insult: 924 / 19170 (4.82%)\n",
      "identity_hate: 157 / 19170 (0.819%)\n",
      "CV train 2\n",
      "        toxic: 7397 / 76681 (9.646%)\n",
      " severe_toxic: 764 / 76681 (0.996%)\n",
      "      obscene: 4072 / 76681 (5.31%)\n",
      "       threat: 248 / 76681 (0.323%)\n",
      "       insult: 3795 / 76681 (4.949%)\n",
      "identity_hate: 651 / 76681 (0.849%)\n",
      "CV test 2\n",
      "        toxic: 1840 / 19170 (9.598%)\n",
      " severe_toxic: 201 / 19170 (1.049%)\n",
      "      obscene: 1037 / 19170 (5.409%)\n",
      "       threat: 57 / 19170 (0.297%)\n",
      "       insult: 970 / 19170 (5.06%)\n",
      "identity_hate: 163 / 19170 (0.85%)\n",
      "CV train 3\n",
      "        toxic: 7392 / 76681 (9.64%)\n",
      " severe_toxic: 769 / 76681 (1.003%)\n",
      "      obscene: 4068 / 76681 (5.305%)\n",
      "       threat: 254 / 76681 (0.331%)\n",
      "       insult: 3818 / 76681 (4.979%)\n",
      "identity_hate: 652 / 76681 (0.85%)\n",
      "CV test 3\n",
      "        toxic: 1845 / 19170 (9.624%)\n",
      " severe_toxic: 196 / 19170 (1.022%)\n",
      "      obscene: 1041 / 19170 (5.43%)\n",
      "       threat: 51 / 19170 (0.266%)\n",
      "       insult: 947 / 19170 (4.94%)\n",
      "identity_hate: 162 / 19170 (0.845%)\n",
      "CV train 4\n",
      "        toxic: 7380 / 76681 (9.624%)\n",
      " severe_toxic: 763 / 76681 (0.995%)\n",
      "      obscene: 4052 / 76681 (5.284%)\n",
      "       threat: 240 / 76681 (0.313%)\n",
      "       insult: 3780 / 76681 (4.93%)\n",
      "identity_hate: 642 / 76681 (0.837%)\n",
      "CV test 4\n",
      "        toxic: 1857 / 19170 (9.687%)\n",
      " severe_toxic: 202 / 19170 (1.054%)\n",
      "      obscene: 1057 / 19170 (5.514%)\n",
      "       threat: 65 / 19170 (0.339%)\n",
      "       insult: 985 / 19170 (5.138%)\n",
      "identity_hate: 172 / 19170 (0.897%)\n"
     ]
    }
   ],
   "source": [
    "def print_count_of_each_label(df):\n",
    "    for label in labels:\n",
    "        print('{}: {} / {} ({}%)'.format(label.rjust(len(labels[-1])),\n",
    "                                         df.loc[df[label] == 1].shape[0],\n",
    "                                         len(df),\n",
    "                                         np.round(df.loc[df[label] == 1].shape[0]/len(df)*100, 3)))\n",
    "\n",
    "for i in range(cv_folds):\n",
    "    print('CV train {}'.format(i))\n",
    "    print_count_of_each_label(cv_train[i])\n",
    "    print('CV test {}'.format(i))\n",
    "    print_count_of_each_label(cv_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test into X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we can use sklearn classifiers easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:47.241326Z",
     "start_time": "2017-12-29T11:50:47.212328Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [None] * cv_folds\n",
    "y_train = [None] * cv_folds\n",
    "X_test = [None] * cv_folds\n",
    "y_test = [None] * cv_folds\n",
    "for i in range(cv_folds):\n",
    "    X_train[i], y_train[i] = train[i][[\"comment_text\"]], train[i][labels]\n",
    "    X_test[i], y_test[i] = test[i][[\"comment_text\"]], test[i][labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:47.456327Z",
     "start_time": "2017-12-29T11:50:47.452327Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 1) (76680, 6)\n",
      "(19171, 1) (19171, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape, y_train[0].shape)\n",
    "print(X_test[0].shape, y_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:48.253489Z",
     "start_time": "2017-12-29T11:50:48.243491Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_textstat_features(df):\n",
    "    features_df = pd.DataFrame()\n",
    "    features_df['comment_text_len'] = df['comment_text'].apply(len)\n",
    "    features_df['comment_text_lex_count'] = df['comment_text'].apply(textstat.lexicon_count)\n",
    "    features_df['comment_text_syl_count'] = df['comment_text'].apply(textstat.syllable_count)\n",
    "    features_df['comment_text_sent_count'] = df['comment_text'].apply(textstat.sentence_count)\n",
    "    features_df['comment_text_flesch_reading_ease'] = df['comment_text'].apply(textstat.flesch_reading_ease)\n",
    "    features_df['comment_text_flesch_kincaid_grade'] = df['comment_text'].apply(textstat.flesch_kincaid_grade)\n",
    "    \n",
    "    features_df['comment_text_syl_over_lex'] = features_df['comment_text_syl_count'] / features_df['comment_text_lex_count']\n",
    "    features_df['comment_text_lex_over_sent'] = features_df['comment_text_lex_count'] / features_df['comment_text_sent_count']\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:48.543487Z",
     "start_time": "2017-12-29T11:50:48.459489Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_textstat_features_file = '../data/train_textstat_features.csv'\n",
    "if os.path.isfile(train_textstat_features_file):\n",
    "    X_train_all_features_textstat = pd.read_csv(train_textstat_features_file, index_col=0)\n",
    "else:\n",
    "    X_train_all_features_textstat = extract_textstat_features(train_all)\n",
    "    X_train_all_features_textstat.to_csv(train_textstat_features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:21:02.663558Z",
     "start_time": "2017-12-25T03:21:02.123907Z"
    }
   },
   "source": [
    "## Split textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:49.183579Z",
     "start_time": "2017-12-29T11:50:49.139581Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_textstat = []\n",
    "X_test_features_textstat = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_train[i], :])\n",
    "    X_test_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:50:49.656626Z",
     "start_time": "2017-12-29T11:50:49.651627Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_tok = re.compile('([{}“”¨«»®´·º½¾¿¡§£₤‘’])'.format(string.punctuation))\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:15.383547Z",
     "start_time": "2017-12-29T11:50:55.404544Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = train_all.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "train_term_doc = vec.fit_transform(train_all['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.182547Z",
     "start_time": "2017-12-29T11:51:15.384547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_bow = []\n",
    "X_test_features_bow = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_bow.append(train_term_doc[cv_splits_train[i], :])\n",
    "    X_test_features_bow.append(train_term_doc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract emotion scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.189550Z",
     "start_time": "2017-12-29T11:51:16.183550Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotion_lexicon_file = \"../data/features/NRC-AffectIntensity-Lexicon.txt\"\n",
    "emotion_lexicon = pd.read_csv(emotion_lexicon_file, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.198573Z",
     "start_time": "2017-12-29T11:51:16.190549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_term_score = {'anger': {}, 'fear': {}, 'joy': {}, 'sadness': {}}\n",
    "for row in emotion_lexicon.itertuples():\n",
    "    emotion_term_score[row.AffectDimension][row.term] = row.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:16.212546Z",
     "start_time": "2017-12-29T11:51:16.199547Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness']\n",
    "def avg_emotion_score(comment_text):\n",
    "    try:\n",
    "        comment_cleaned = comment_text.translate(translator)\n",
    "    except:\n",
    "        comment_cleaned = \"\"\n",
    "    comment_cleaned = comment_cleaned.lower()\n",
    "    comment_cleaned_words = comment_cleaned.split(\" \")\n",
    "    \n",
    "    emotion_scores = {'anger': 0, 'fear': 0, 'joy': 0, 'sadness': 0}\n",
    "    for emotion in emotions:\n",
    "        scores = [emotion_term_score[emotion].get(word) for word in comment_cleaned_words\n",
    "                    if emotion_term_score[emotion].get(word) is not None]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        emotion_scores[emotion] = np.sum(scores) / len(comment_cleaned_words)\n",
    "        \n",
    "    return [emotion_scores[emotion] for emotion in emotions]\n",
    "    \n",
    "def extract_emotion_features(df):\n",
    "    features_df = df['comment_text'].apply(avg_emotion_score)\n",
    "    return pd.DataFrame(features_df.values.tolist(), columns=['comment_text_emotion_{}'.format(emotion) for emotion in emotions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:21.960189Z",
     "start_time": "2017-12-29T11:51:16.213549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_all_features_emotion = extract_emotion_features(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:22.001189Z",
     "start_time": "2017-12-29T11:51:21.961192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_emotion = []\n",
    "X_test_features_emotion = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_emotion.append(X_train_all_features_emotion.loc[cv_splits_train[i], :])\n",
    "    X_test_features_emotion.append(X_train_all_features_emotion.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:01:28.405401Z",
     "start_time": "2017-12-29T11:01:28.402400Z"
    }
   },
   "source": [
    "## Extract Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:50.085194Z",
     "start_time": "2017-12-29T11:51:22.002192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:51:50.091195Z",
     "start_time": "2017-12-29T11:51:50.086197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_w2v(comment_text):\n",
    "    sentence = tokenize(comment_text)\n",
    "    f = np.zeros(w2v_model.vector_size)\n",
    "    count = 0 \n",
    "    for w in sentence:\n",
    "        try:\n",
    "            vec = w2v_model[w]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "        f += vec\n",
    "    if count > 0:\n",
    "        f /= count\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:13.850198Z",
     "start_time": "2017-12-29T11:51:50.092195Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v = train_all['comment_text'].apply(featurize_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:13.977199Z",
     "start_time": "2017-12-29T11:52:13.851200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v = np.array(X_train_all_features_w2v.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.271199Z",
     "start_time": "2017-12-29T11:52:13.978200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_w2v = []\n",
    "X_test_features_w2v = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_w2v.append(X_train_all_features_w2v[cv_splits_train[i]])\n",
    "    X_test_features_w2v.append(X_train_all_features_w2v[cv_splits_test[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract glove Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_glove_model = gensim.models.KeyedVectors.load_word2vec_format('../data/glove.6B.50d.gensim.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_w2v_glove(comment_text):\n",
    "    sentence = tokenize(comment_text)\n",
    "    f = np.zeros(w2v_glove_model.vector_size)\n",
    "    count = 0 \n",
    "    for w in sentence:\n",
    "        try:\n",
    "            vec = w2v_glove_model[w]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "        f += vec\n",
    "    if count > 0:\n",
    "        f /= count\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v_glove = train_all['comment_text'].apply(featurize_w2v_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v_glove = np.array(X_train_all_features_w2v_glove.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features_w2v_glove = []\n",
    "X_test_features_w2v_glove = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_w2v_glove.append(X_train_all_features_w2v_glove[cv_splits_train[i]])\n",
    "    X_test_features_w2v_glove.append(X_train_all_features_w2v_glove[cv_splits_test[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to calculate mean column-wise log loss of y_pred vs y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.275203Z",
     "start_time": "2017-12-29T11:52:14.272199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_actual, y_pred):\n",
    "    col_log_loss = [log_loss(np.array(y_actual[label]),\n",
    "                             np.array([1.-np.array(y_pred[label]), np.array(y_pred[label])]).T) for label in labels]\n",
    "    return col_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classification scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.301199Z",
     "start_time": "2017-12-29T11:52:14.276203Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9920072216264108e-16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(calculate_score(y_test[0], y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.397202Z",
     "start_time": "2017-12-29T11:52:14.302201Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.32844347  0.34772673  1.84096823  0.10990296  1.71701177  0.29331534]\n",
      "1.27289475201\n"
     ]
    }
   ],
   "source": [
    "scores_zeror = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.zeros(len(labels))] * len(X_test[i]))\n",
    "    y_pred_zeror = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_zeror)\n",
    "    scores_zeror.append(scores)\n",
    "print(np.mean(scores_zeror, axis=0))\n",
    "print(np.mean(scores_zeror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-24T12:28:47.459632Z",
     "start_time": "2017-12-24T12:28:47.455633Z"
    }
   },
   "source": [
    "## All 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.490201Z",
     "start_time": "2017-12-29T11:52:14.398201Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69314718  0.69314718  0.69314718  0.69314718  0.69314718  0.69314718]\n",
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "scores_half = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.ones(len(labels))*0.5] * len(X_test[i]))\n",
    "    y_pred_half = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_half)\n",
    "    scores_half.append(scores)\n",
    "print(np.mean(scores_half, axis=0))\n",
    "print(np.mean(scores_half))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test calculate_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.588202Z",
     "start_time": "2017-12-29T11:52:14.491203Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.69314718   0.91220862   1.15881057   1.6050267    2.19335539\n",
      "  34.24546105]\n",
      "6.80133491769\n"
     ]
    }
   ],
   "source": [
    "scores_half = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])] * len(X_test[i]))\n",
    "    y_pred_half = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_half)\n",
    "    scores_half.append(scores)\n",
    "print(np.mean(scores_half, axis=0))\n",
    "print(np.mean(scores_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.594202Z",
     "start_time": "2017-12-29T11:52:14.589200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_classifier_per_label(classifiers, y_pred, scores):\n",
    "    classifier_scores = np.array([np.mean(scores[classifier_name], axis=0) for classifier_name, _ in classifiers])\n",
    "    best_classifier_score_per_label = [(classifiers[min_idx][0], classifier_scores[min_idx][i])\n",
    "                                           for i, min_idx in enumerate(np.argmin(classifier_scores, axis=0))]\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\"[{}] {} : {}\".format(label, best_classifier_score_per_label[i][0], best_classifier_score_per_label[i][1]))\n",
    "    print(\"Average: {}\".format(np.mean([x[1] for x in best_classifier_score_per_label])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:52:14.603200Z",
     "start_time": "2017-12-29T11:52:14.595201Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classifiers(classifiers, X_train_features, X_test_features, y_train, clf, y_pred, scores):\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        print('Training with {}'.format(classifier_name))\n",
    "        clf[classifier_name] = [{}]*cv_folds\n",
    "        y_pred[classifier_name] = [{}]*cv_folds\n",
    "        scores[classifier_name] = [{}]*cv_folds\n",
    "        for fold in range(cv_folds):\n",
    "            y_pred[classifier_name][fold] = pd.DataFrame()\n",
    "            for label in labels:\n",
    "                clf[classifier_name][fold][label] = classifier()\n",
    "                clf[classifier_name][fold][label].fit(X_train_features[fold], y_train[fold][label])\n",
    "\n",
    "                y_pred[classifier_name][fold][label] = clf[classifier_name][fold][label].predict_proba(X_test_features[fold]).T[1]\n",
    "\n",
    "            scores[classifier_name][fold] = calculate_score(y_test[fold], y_pred[classifier_name][fold])\n",
    "        print('Column-wise log loss for {}: {} - {}'.format(classifier_name, np.mean(scores[classifier_name], axis=0), np.mean(scores[classifier_name])))\n",
    "    best_classifier_per_label(classifiers, y_pred, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:26:24.881527Z",
     "start_time": "2017-12-29T12:26:24.871025Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_textstat = []\n",
    "# classifiers_textstat += [\n",
    "#     ('Logistic Regression {}'.format(c), lambda: LogisticRegression(solver=\"newton-cg\", C=2.0**c, max_iter=1000)) for c in np.arange(-3,4,1)]\n",
    "classifiers_textstat += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "#     ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "#     (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "#     (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "#     (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "#     (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "#     (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "#     (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "#     (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "    # Cannot even run\n",
    "    #     (\"Gaussian Process\", lambda: GaussianProcessClassifier(1.0 * RBF(1.0)))  # Memory error??? Even with 32GB ram???\n",
    "    #     (\"Linear SVM\", lambda: SVC(kernel=\"linear\", C=0.025)),                   # Slow like shit\n",
    "    #     (\"RBF SVM\", lambda: SVC(gamma=2, C=1))                                   # Slow like shit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:32:36.401372Z",
     "start_time": "2017-12-29T12:26:25.061817Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.28483668  0.04888504  0.18583719  0.01931983  0.17817057  0.04614669] - 0.12719933169122624\n",
      "[toxic] XGBoost : 0.284836675541959\n",
      "[severe_toxic] XGBoost : 0.04888503970659461\n",
      "[obscene] XGBoost : 0.18583719111278976\n",
      "[threat] XGBoost : 0.0193198264879471\n",
      "[insult] XGBoost : 0.17817056803190126\n",
      "[identity_hate] XGBoost : 0.04614668926616587\n",
      "Average: 0.12719933169122624\n"
     ]
    }
   ],
   "source": [
    "clf_textstat = {}\n",
    "y_pred_textstat = {}\n",
    "scores_textstat = {}\n",
    "run_classifiers(classifiers_textstat, X_train_features_textstat, X_test_features_textstat, y_train,\n",
    "                clf_textstat, y_pred_textstat, scores_textstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:32:36.410873Z",
     "start_time": "2017-12-29T12:32:36.402372Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_emotion = []\n",
    "classifiers_emotion += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "#     ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "#     (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "#     (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "#     (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "#     (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "#     (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "#     (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "#     (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:37:57.589171Z",
     "start_time": "2017-12-29T12:32:36.411873Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.27910365  0.04835497  0.18336427  0.01567885  0.17384561  0.04629481] - 0.12444035929347044\n",
      "[toxic] XGBoost : 0.27910365378493474\n",
      "[severe_toxic] XGBoost : 0.048354966282939324\n",
      "[obscene] XGBoost : 0.18336426875087616\n",
      "[threat] XGBoost : 0.01567884675641421\n",
      "[insult] XGBoost : 0.17384561448144115\n",
      "[identity_hate] XGBoost : 0.046294805704217123\n",
      "Average: 0.12444035929347046\n"
     ]
    }
   ],
   "source": [
    "clf_emotion = {}\n",
    "y_pred_emotion = {}\n",
    "scores_emotion = {}\n",
    "run_classifiers(classifiers_emotion, X_train_features_emotion, X_test_features_emotion, y_train,\n",
    "                clf_emotion, y_pred_emotion, scores_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat + emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:37:57.600671Z",
     "start_time": "2017-12-29T12:37:57.590171Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_textstat_emotion = []\n",
    "classifiers_textstat_emotion += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "#     ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "#     (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "#     (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "#     (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "#     (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "#     (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "#     (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "#     (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:37:57.635671Z",
     "start_time": "2017-12-29T12:37:57.601671Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 8)\n",
      "(76680, 4)\n",
      "(76680, 12)\n"
     ]
    }
   ],
   "source": [
    "# concat features\n",
    "X_train_features_textstat_emotion = [None]*cv_folds\n",
    "X_test_features_textstat_emotion = [None]*cv_folds\n",
    "for fold in range(cv_folds):\n",
    "    X_train_features_textstat_emotion[fold] = np.hstack((X_train_features_textstat[fold], X_train_features_emotion[fold].values))\n",
    "    X_test_features_textstat_emotion[fold] = np.hstack((X_test_features_textstat[fold], X_test_features_emotion[fold].values))\n",
    "\n",
    "print(X_train_features_textstat[0].shape)\n",
    "print(X_train_features_emotion[0].shape)\n",
    "print(X_train_features_textstat_emotion[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:46:31.586121Z",
     "start_time": "2017-12-29T12:37:57.636671Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.25704964  0.04406231  0.16881971  0.01425027  0.16175613  0.04428455] - 0.1150371041726767\n",
      "[toxic] XGBoost : 0.25704964456713714\n",
      "[severe_toxic] XGBoost : 0.04406231405357034\n",
      "[obscene] XGBoost : 0.16881971271279\n",
      "[threat] XGBoost : 0.014250270017938266\n",
      "[insult] XGBoost : 0.16175613442820064\n",
      "[identity_hate] XGBoost : 0.04428454925642382\n",
      "Average: 0.11503710417267671\n"
     ]
    }
   ],
   "source": [
    "clf_textstat_emotion = {}\n",
    "y_pred_textstat_emotion = {}\n",
    "scores_textstat_emotion = {}\n",
    "run_classifiers(classifiers_textstat_emotion, X_train_features_textstat_emotion,\n",
    "                X_test_features_textstat_emotion, y_train,\n",
    "                clf_textstat_emotion, y_pred_textstat_emotion, scores_textstat_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:46:31.591121Z",
     "start_time": "2017-12-29T12:46:31.587120Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_bow = [\n",
    "#     (\"Multinominal Naive Bayes\", lambda: MultinomialNB()),\n",
    "#     ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T13:44:57.238874Z",
     "start_time": "2017-12-29T12:46:31.592122Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with NB-SVM\n",
      "Column-wise log loss for NB-SVM: [ 0.10984432  0.02989424  0.06178523  0.01158265  0.07788502  0.0280709 ] - 0.05317706064445081\n",
      "[toxic] NB-SVM : 0.1098443236820131\n",
      "[severe_toxic] NB-SVM : 0.029894241779756425\n",
      "[obscene] NB-SVM : 0.06178523018708384\n",
      "[threat] NB-SVM : 0.01158264798630578\n",
      "[insult] NB-SVM : 0.0778850242041904\n",
      "[identity_hate] NB-SVM : 0.028070896027355303\n",
      "Average: 0.0531770606444508\n"
     ]
    }
   ],
   "source": [
    "clf_bow = {}\n",
    "y_pred_bow = {}\n",
    "scores_bow = {}\n",
    "run_classifiers(classifiers_bow, X_train_features_bow,\n",
    "                X_test_features_bow, y_train,\n",
    "                clf_bow, y_pred_bow, scores_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:15:41.929400Z",
     "start_time": "2017-12-29T12:15:41.925900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_w2v = [\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:18:36.896419Z",
     "start_time": "2017-12-29T12:15:43.118302Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression\n",
      "Column-wise log loss for Logistic Regression: [ 0.14969022  0.03185584  0.09973018  0.0133629   0.10659322  0.03318054] - 0.07240214930700903\n",
      "[toxic] Logistic Regression : 0.14969021900629492\n",
      "[severe_toxic] Logistic Regression : 0.03185584269411572\n",
      "[obscene] Logistic Regression : 0.09973017523216073\n",
      "[threat] Logistic Regression : 0.013362904976916574\n",
      "[insult] Logistic Regression : 0.1065932172309775\n",
      "[identity_hate] Logistic Regression : 0.033180536701588756\n",
      "Average: 0.07240214930700903\n"
     ]
    }
   ],
   "source": [
    "clf_w2v = {}\n",
    "y_pred_w2v = {}\n",
    "scores_w2v = {}\n",
    "\n",
    "run_classifiers(classifiers_w2v, X_train_features_w2v,\n",
    "                X_test_features_w2v, y_train,\n",
    "                clf_w2v, y_pred_w2v, scores_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec glove only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:15:41.929400Z",
     "start_time": "2017-12-29T12:15:41.925900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_w2v_glove = [\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:18:36.896419Z",
     "start_time": "2017-12-29T12:15:43.118302Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression\n",
      "Column-wise log loss for Logistic Regression: [ 0.21309508  0.04024206  0.14121731  0.01670117  0.13662473  0.03881534] - 0.09778261574609753\n",
      "[toxic] Logistic Regression : 0.21309507787243942\n",
      "[severe_toxic] Logistic Regression : 0.040242061860483166\n",
      "[obscene] Logistic Regression : 0.14121730655828618\n",
      "[threat] Logistic Regression : 0.016701174299176898\n",
      "[insult] Logistic Regression : 0.13662473380238926\n",
      "[identity_hate] Logistic Regression : 0.03881534008381026\n",
      "Average: 0.09778261574609753\n"
     ]
    }
   ],
   "source": [
    "clf_w2v_glove = {}\n",
    "y_pred_w2v_glove = {}\n",
    "scores_w2v_glove = {}\n",
    "\n",
    "run_classifiers(classifiers_w2v_glove, X_train_features_w2v_glove,\n",
    "                X_test_features_w2v_glove, y_train,\n",
    "                clf_w2v_glove, y_pred_w2v_glove, scores_w2v_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:20:20.481155Z",
     "start_time": "2017-12-29T12:20:20.478154Z"
    }
   },
   "source": [
    "# Average results from multiple features and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:12:59.934863Z",
     "start_time": "2017-12-29T16:12:59.929864Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_predictions(y_preds, classifier_names, weights):\n",
    "    normalized_weights = np.array(weights) / np.sum(weights)\n",
    "    y_pred_avg = [None] * cv_folds\n",
    "    \n",
    "    for i, fold in enumerate(range(cv_folds)):\n",
    "        for i, y_pred in enumerate(y_preds):\n",
    "            if i==0:\n",
    "                y_pred_avg[fold] = y_pred[classifier_names[i]][fold]*weights[i]\n",
    "            else:\n",
    "                y_pred_avg[fold] += y_pred[classifier_names[i]][fold]*weights[i]\n",
    "    return calculate_score(y_test[fold], y_pred_avg[fold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat+emotion + BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:13:00.588929Z",
     "start_time": "2017-12-29T16:13:00.561933Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15449181633605233, 0.031560525357768703, 0.092664343353911063, 0.012277951374001441, 0.10168048385854131, 0.032841573882729395]\n",
      "0.0709194490272\n"
     ]
    }
   ],
   "source": [
    "textstat_emotion_AVG_bow_score = average_predictions([y_pred_textstat_emotion, y_pred_bow],\n",
    "                                                     [\"XGBoost\", \"NB-SVM\"],\n",
    "                                                     [0.5, 0.5])\n",
    "print(textstat_emotion_AVG_bow_score)\n",
    "print(np.mean(textstat_emotion_AVG_bow_score))\n",
    "\n",
    "\n",
    "# 0.11366952025039936\n",
    "# 0.053176833214058444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat+emotion + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:13:04.388452Z",
     "start_time": "2017-12-29T16:13:04.361455Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17380224597115454, 0.034570600071424248, 0.11705944494758774, 0.013036789672118227, 0.11878032058068376, 0.036281322589013093]\n",
      "0.0822551206387\n"
     ]
    }
   ],
   "source": [
    "textstat_emotion_AVG_w2v_score = average_predictions([y_pred_textstat_emotion, y_pred_w2v],\n",
    "                                                     [\"XGBoost\", \"Logistic Regression\"],\n",
    "                                                     [0.5, 0.5])\n",
    "print(textstat_emotion_AVG_w2v_score)\n",
    "print(np.mean(textstat_emotion_AVG_w2v_score))\n",
    "\n",
    "\n",
    "# 0.11366952025039936\n",
    "# 0.07266043582520464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:18:06.341376Z",
     "start_time": "2017-12-29T16:18:06.313376Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10556171529822136, 0.027286902212003302, 0.06117745396973194, 0.011202445642929175, 0.077549464425811843, 0.028430402620803086]\n",
      "0.0518680640283\n"
     ]
    }
   ],
   "source": [
    "bow_AVG_w2v_score = average_predictions([y_pred_bow, y_pred_w2v],\n",
    "                                        [\"NB-SVM\", \"Logistic Regression\"],\n",
    "                                        [0.9, 0.1])\n",
    "print(bow_AVG_w2v_score)\n",
    "print(np.mean(bow_AVG_w2v_score))\n",
    "\n",
    "# 0.053176833214058444\n",
    "# 0.07266043582520464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW + Word2Vec + textstat+emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T16:19:23.232456Z",
     "start_time": "2017-12-29T16:19:23.201455Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10558159526655088, 0.027285213606180359, 0.062445173643995523, 0.01120044290915395, 0.077625754272640332, 0.028431435235483218]\n",
      "0.0520949358223\n"
     ]
    }
   ],
   "source": [
    "bow_AVG_w2v_AVG_textstat_emotion_score = average_predictions([y_pred_bow, y_pred_w2v, y_pred_textstat_emotion],\n",
    "                                        [\"NB-SVM\", \"Logistic Regression\", \"XGBoost\"],\n",
    "                                        [0.9, 0.1, 0.001])\n",
    "print(bow_AVG_w2v_AVG_textstat_emotion_score)\n",
    "print(np.mean(bow_AVG_w2v_AVG_textstat_emotion_score))\n",
    "\n",
    "# 0.053176833214058444\n",
    "# 0.07266043582520464\n",
    "# 0.11366952025039936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classifiers(classifiers, X_train_features, X_test_features, y_train, clf, y_pred, scores):\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        print('Training with {}'.format(classifier_name))\n",
    "        clf[classifier_name] = [{}]*cv_folds\n",
    "        y_pred[classifier_name] = [{}]*cv_folds\n",
    "        scores[classifier_name] = [{}]*cv_folds\n",
    "        for fold in range(cv_folds):\n",
    "            y_pred[classifier_name][fold] = pd.DataFrame()\n",
    "            for label in labels:\n",
    "                clf[classifier_name][fold][label] = classifier()\n",
    "                clf[classifier_name][fold][label].fit(X_train_features[fold], y_train[fold][label])\n",
    "\n",
    "                y_pred[classifier_name][fold][label] = clf[classifier_name][fold][label].predict_proba(X_test_features[fold]).T[1]\n",
    "\n",
    "            scores[classifier_name][fold] = calculate_score(y_test[fold], y_pred[classifier_name][fold])\n",
    "        print('Column-wise log loss for {}: {} - {}'.format(classifier_name, np.mean(scores[classifier_name], axis=0), np.mean(scores[classifier_name])))\n",
    "    best_classifier_per_label(classifiers, y_pred, scores)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(clf, classifier_name, X_test_features, fold=0):\n",
    "    y_pred_test = []\n",
    "    for label in labels:\n",
    "        y_pred_test.append(clf[classifier_name][fold][label].predict_proba(X_test_features).T[1])\n",
    "    return pd.DataFrame(np.array(y_pred_test).T, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_testa_features_bow = vec.transform(test_for_submission['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_testa_features_w2v = test_for_submission['comment_text'].apply(featurize_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_testa_features_w2v = np.array(X_testa_features_w2v.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 285100)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testa_features_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 300)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testa_features_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow = predict(clf_bow, \"NB-SVM\", X_testa_features_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_test_w2v = predict(clf_w2v, \"Logistic Regression\", X_testa_features_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow[\"id\"] = test_for_submission[\"id\"]\n",
    "y_pred_test_w2v[\"id\"] = test_for_submission[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow = y_pred_test_bow[[\"id\"] + labels]\n",
    "y_pred_test_w2v = y_pred_test_w2v[[\"id\"] + labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow.to_csv(\"submission_bow.csv\", index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test_w2v.to_csv(\"submission_w2v.csv\", index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.001622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.001454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.039752</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>0.011170</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.001210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.001528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0   6044863  0.012127      0.001585  0.005665  0.000498  0.005277   \n",
       "1   6102620  0.009424      0.000887  0.006853  0.000426  0.005164   \n",
       "2  14563293  0.004883      0.001114  0.003772  0.000485  0.003223   \n",
       "3  21086297  0.039752      0.002650  0.011170  0.000468  0.011503   \n",
       "4  22982444  0.010744      0.002018  0.006046  0.000539  0.004098   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.001622  \n",
       "1       0.001454  \n",
       "2       0.000956  \n",
       "3       0.001210  \n",
       "4       0.001528  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.009607</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.013383</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.004393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.004856</td>\n",
       "      <td>0.002553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.000629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.018521</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.005221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0   6044863  0.009607      0.006013  0.013383  0.000152  0.012652   \n",
       "1   6102620  0.007159      0.001165  0.003517  0.000453  0.004856   \n",
       "2  14563293  0.008073      0.001183  0.005025  0.000245  0.006145   \n",
       "3  21086297  0.000440      0.000069  0.000016  0.000020  0.000372   \n",
       "4  22982444  0.004941      0.018521  0.002493  0.000103  0.006026   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.004393  \n",
       "1       0.002553  \n",
       "2       0.000629  \n",
       "3       0.000163  \n",
       "4       0.005221  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_w2v.head()  0.012127*0.9 + 0.009607*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow_w2v = y_pred_test_bow.loc[:, labels] *0.9 + y_pred_test_w2v.loc[:, labels] *0.1\n",
    "\n",
    "\n",
    "y_pred_test_bow_w2v[\"id\"] = test_for_submission[\"id\"]\n",
    "\n",
    "y_pred_test_bow_w2v = y_pred_test_bow_w2v[[\"id\"] + labels]\n",
    "\n",
    "y_pred_test_bow_w2v.to_csv(\"submission_bow_w2v.csv\", index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 7)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_bow_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
