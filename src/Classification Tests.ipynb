{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:18.500088Z",
     "start_time": "2017-12-25T03:48:17.949090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# BoW feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.035966Z",
     "start_time": "2017-12-25T03:48:18.501089Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "test_file = '../data/test.csv'\n",
    "sample_submission_file = '../data/sample_submission.csv'\n",
    "\n",
    "train_all = pd.read_csv(train_file)\n",
    "test_for_submission = pd.read_csv(test_file)\n",
    "sample_submission = pd.read_csv(sample_submission_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.049468Z",
     "start_time": "2017-12-25T03:48:20.036969Z"
    }
   },
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits=5, test_size=.2, random_state=0)\n",
    "cv_splits_train = []\n",
    "cv_splits_test = []\n",
    "for train_index, test_index in rs.split(train_all):\n",
    "    cv_splits_train.append(train_index)\n",
    "    cv_splits_test.append(test_index)\n",
    "cv_folds = len(cv_splits_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.126467Z",
     "start_time": "2017-12-25T03:48:20.050470Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_train = []\n",
    "cv_test = []\n",
    "for i in range(cv_folds):\n",
    "    cv_train.append(train_all.loc[cv_splits_train[i], :])\n",
    "    cv_test.append(train_all.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.131967Z",
     "start_time": "2017-12-25T03:48:20.127467Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = cv_train\n",
    "test = cv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and check that the data is split such that the percentages of labels in train/test are roughly equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.139967Z",
     "start_time": "2017-12-25T03:48:20.132968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.230468Z",
     "start_time": "2017-12-25T03:48:20.140968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV train 0\n",
      "        toxic: 7396 / 76680 (9.645%)\n",
      " severe_toxic: 768 / 76680 (1.002%)\n",
      "      obscene: 4073 / 76680 (5.312%)\n",
      "       threat: 251 / 76680 (0.327%)\n",
      "       insult: 3831 / 76680 (4.996%)\n",
      "identity_hate: 641 / 76680 (0.836%)\n",
      "CV test 0\n",
      "        toxic: 1841 / 19171 (9.603%)\n",
      " severe_toxic: 197 / 19171 (1.028%)\n",
      "      obscene: 1036 / 19171 (5.404%)\n",
      "       threat: 54 / 19171 (0.282%)\n",
      "       insult: 934 / 19171 (4.872%)\n",
      "identity_hate: 173 / 19171 (0.902%)\n",
      "CV train 1\n",
      "        toxic: 7326 / 76680 (9.554%)\n",
      " severe_toxic: 771 / 76680 (1.005%)\n",
      "      obscene: 4067 / 76680 (5.304%)\n",
      "       threat: 256 / 76680 (0.334%)\n",
      "       insult: 3786 / 76680 (4.937%)\n",
      "identity_hate: 646 / 76680 (0.842%)\n",
      "CV test 1\n",
      "        toxic: 1911 / 19171 (9.968%)\n",
      " severe_toxic: 194 / 19171 (1.012%)\n",
      "      obscene: 1042 / 19171 (5.435%)\n",
      "       threat: 49 / 19171 (0.256%)\n",
      "       insult: 979 / 19171 (5.107%)\n",
      "identity_hate: 168 / 19171 (0.876%)\n",
      "CV train 2\n",
      "        toxic: 7377 / 76680 (9.621%)\n",
      " severe_toxic: 754 / 76680 (0.983%)\n",
      "      obscene: 4100 / 76680 (5.347%)\n",
      "       threat: 246 / 76680 (0.321%)\n",
      "       insult: 3793 / 76680 (4.947%)\n",
      "identity_hate: 641 / 76680 (0.836%)\n",
      "CV test 2\n",
      "        toxic: 1860 / 19171 (9.702%)\n",
      " severe_toxic: 211 / 19171 (1.101%)\n",
      "      obscene: 1009 / 19171 (5.263%)\n",
      "       threat: 59 / 19171 (0.308%)\n",
      "       insult: 972 / 19171 (5.07%)\n",
      "identity_hate: 173 / 19171 (0.902%)\n",
      "CV train 3\n",
      "        toxic: 7398 / 76680 (9.648%)\n",
      " severe_toxic: 763 / 76680 (0.995%)\n",
      "      obscene: 4096 / 76680 (5.342%)\n",
      "       threat: 242 / 76680 (0.316%)\n",
      "       insult: 3792 / 76680 (4.945%)\n",
      "identity_hate: 661 / 76680 (0.862%)\n",
      "CV test 3\n",
      "        toxic: 1839 / 19171 (9.593%)\n",
      " severe_toxic: 202 / 19171 (1.054%)\n",
      "      obscene: 1013 / 19171 (5.284%)\n",
      "       threat: 63 / 19171 (0.329%)\n",
      "       insult: 973 / 19171 (5.075%)\n",
      "identity_hate: 153 / 19171 (0.798%)\n",
      "CV train 4\n",
      "        toxic: 7370 / 76680 (9.611%)\n",
      " severe_toxic: 777 / 76680 (1.013%)\n",
      "      obscene: 4061 / 76680 (5.296%)\n",
      "       threat: 241 / 76680 (0.314%)\n",
      "       insult: 3811 / 76680 (4.97%)\n",
      "identity_hate: 670 / 76680 (0.874%)\n",
      "CV test 4\n",
      "        toxic: 1867 / 19171 (9.739%)\n",
      " severe_toxic: 188 / 19171 (0.981%)\n",
      "      obscene: 1048 / 19171 (5.467%)\n",
      "       threat: 64 / 19171 (0.334%)\n",
      "       insult: 954 / 19171 (4.976%)\n",
      "identity_hate: 144 / 19171 (0.751%)\n"
     ]
    }
   ],
   "source": [
    "def print_count_of_each_label(df):\n",
    "    for label in labels:\n",
    "        print('{}: {} / {} ({}%)'.format(label.rjust(len(labels[-1])),\n",
    "                                         df.loc[df[label] == 1].shape[0],\n",
    "                                         len(df),\n",
    "                                         np.round(df.loc[df[label] == 1].shape[0]/len(df)*100, 3)))\n",
    "\n",
    "for i in range(cv_folds):\n",
    "    print('CV train {}'.format(i))\n",
    "    print_count_of_each_label(cv_train[i])\n",
    "    print('CV test {}'.format(i))\n",
    "    print_count_of_each_label(cv_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test into X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we can use sklearn classifiers easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.264967Z",
     "start_time": "2017-12-25T03:48:20.231468Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = [None] * cv_folds\n",
    "y_train = [None] * cv_folds\n",
    "X_test = [None] * cv_folds\n",
    "y_test = [None] * cv_folds\n",
    "for i in range(cv_folds):\n",
    "    X_train[i], y_train[i] = train[i][[\"comment_text\"]], train[i][labels]\n",
    "    X_test[i], y_test[i] = test[i][[\"comment_text\"]], test[i][labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.269969Z",
     "start_time": "2017-12-25T03:48:20.265967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 1) (76680, 6)\n",
      "(19171, 1) (19171, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape, y_train[0].shape)\n",
    "print(X_test[0].shape, y_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.279968Z",
     "start_time": "2017-12-25T03:48:20.271469Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    features_df = pd.DataFrame()\n",
    "    features_df['comment_text_len'] = df['comment_text'].apply(len)\n",
    "    features_df['comment_text_lex_count'] = df['comment_text'].apply(textstat.lexicon_count)\n",
    "    features_df['comment_text_syl_count'] = df['comment_text'].apply(textstat.syllable_count)\n",
    "    features_df['comment_text_sent_count'] = df['comment_text'].apply(textstat.sentence_count)\n",
    "    features_df['comment_text_flesch_reading_ease'] = df['comment_text'].apply(textstat.flesch_reading_ease)\n",
    "    features_df['comment_text_flesch_kincaid_grade'] = df['comment_text'].apply(textstat.flesch_kincaid_grade)\n",
    "    \n",
    "    features_df['comment_text_syl_over_lex'] = features_df['comment_text_syl_count'] / features_df['comment_text_lex_count']\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.350467Z",
     "start_time": "2017-12-25T03:48:20.281468Z"
    }
   },
   "outputs": [],
   "source": [
    "textstat_features_file = '../data/textstat_features.csv'\n",
    "if os.path.isfile(textstat_features_file):\n",
    "    X_train_all_features_textstat = pd.read_csv(textstat_features_file)\n",
    "else:\n",
    "    X_train_all_features_textstat = extract_features(train_all)\n",
    "    X_train_all_features_textstat.to_csv(textstat_features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:21:02.663558Z",
     "start_time": "2017-12-25T03:21:02.123907Z"
    }
   },
   "source": [
    "## Split textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.404967Z",
     "start_time": "2017-12-25T03:48:20.351468Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_features_textstat = []\n",
    "X_test_features_textstat = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_train[i], :])\n",
    "    X_test_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to calculate mean column-wise log loss of y_pred vs y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.408966Z",
     "start_time": "2017-12-25T03:48:20.405967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_actual, y_pred):\n",
    "    return np.mean([log_loss(np.array(y_actual[label]),\n",
    "                             np.array([1.-np.array(y_pred[label]), np.array(y_pred[label])]).T) for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classification scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.436968Z",
     "start_time": "2017-12-25T03:48:20.409970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9920072216264108e-16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_score(y_test[0], y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.529967Z",
     "start_time": "2017-12-25T03:48:20.437968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2716404815645752, 1.304069565864215, 1.2863536772190416, 1.2740426359571411, 1.2806485605366973]\n",
      "1.28335098423\n"
     ]
    }
   ],
   "source": [
    "scores_zeror = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.zeros(len(labels))] * len(X_test[i]))\n",
    "    y_pred_zeror = pd.DataFrame(data, columns=labels)\n",
    "    scores_zeror.append(calculate_score(y_test[i], y_pred_zeror))\n",
    "print(scores_zeror)\n",
    "print(np.mean(scores_zeror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-24T12:28:47.459632Z",
     "start_time": "2017-12-24T12:28:47.455633Z"
    }
   },
   "source": [
    "## All 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.648967Z",
     "start_time": "2017-12-25T03:48:20.530969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69314718055994529, 0.69314718055994529, 0.69314718055994529, 0.69314718055994529, 0.69314718055994529]\n",
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "scores_half = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.ones(len(labels))*0.5] * len(X_test[i]))\n",
    "    y_pred_half = pd.DataFrame(data, columns=labels)\n",
    "    scores_half.append(calculate_score(y_test[i], y_pred_half))\n",
    "print(scores_half)\n",
    "print(np.mean(scores_half))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat features only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through a bunch of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:48:20.655468Z",
     "start_time": "2017-12-25T03:48:20.649967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"Nearest Neighbors\", KNeighborsClassifier(131)),\n",
    "    (\"Naive Bayes\", GaussianNB()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(max_depth=5)),\n",
    "    (\"Random Forest\", RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "    (\"Neural Net\", MLPClassifier(alpha=1)),\n",
    "    (\"AdaBoost\", AdaBoostClassifier()),\n",
    "    (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "    #     (\"Gaussian Process\", GaussianProcessClassifier(1.0 * RBF(1.0)))  # Memory error??? Even with 32GB ram???\n",
    "    #     (\"Linear SVM\", SVC(kernel=\"linear\", C=0.025)),                   # Slow as shit\n",
    "    #     (\"RBF SVM\", SVC(gamma=2, C=1))                                   # Slow as shit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:50:34.660493Z",
     "start_time": "2017-12-25T03:48:20.656467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Nearest Neighbors\n",
      "Column-wise log loss for Nearest Neighbors: [0.5256809621385633, 0.52571879365431262, 0.5176337468898744, 0.5064720242972145, 0.51618901691386421] - 0.5183389087787658\n",
      "Training with Naive Bayes\n",
      "Column-wise log loss for Naive Bayes: [0.67802880910037111, 0.75718086146898489, 0.72469968356287995, 0.67596525454034795, 0.71034136466691755] - 0.7092431946679003\n",
      "Training with Decision Tree\n",
      "Column-wise log loss for Decision Tree: [0.17948596754211299, 0.18728442467369955, 0.18341879999086066, 0.18183658030306449, 0.17913390401916163] - 0.18223193530577986\n",
      "Training with Random Forest\n",
      "Column-wise log loss for Random Forest: [0.17470317728286464, 0.17875249356071574, 0.17497159213817096, 0.17268445506654065, 0.17346514054694429] - 0.17491537171904725\n",
      "Training with Neural Net\n",
      "Column-wise log loss for Neural Net: [1.2705339326373306, 1.3008524708637486, 1.2877383233392219, 1.2715774636027959, 1.2803600643980317] - 1.2822124509682258\n",
      "Training with AdaBoost\n",
      "Column-wise log loss for AdaBoost: [0.64376633259383054, 0.64391267236714422, 0.64430586775379994, 0.64485567122407805, 0.64517968392999892] - 0.6444040455737704\n",
      "Training with QDA\n",
      "Column-wise log loss for QDA: [0.3953550253182665, 0.48706311675166303, 0.49839750966454804, 0.48069069988131047, 0.47127370790630857] - 0.4665560119044193\n"
     ]
    }
   ],
   "source": [
    "clf = {}\n",
    "y_pred = {}\n",
    "scores_textstat = {}\n",
    "for classifier_name, classifier in classifiers:\n",
    "    print('Training with {}'.format(classifier_name))\n",
    "    clf[classifier_name] = [{}]*cv_folds\n",
    "    y_pred[classifier_name] = [{}]*cv_folds\n",
    "    scores_textstat[classifier_name] = [{}]*cv_folds\n",
    "    for fold in range(cv_folds):\n",
    "        for label in labels:\n",
    "            clf[classifier_name][fold][label] = classifier\n",
    "            clf[classifier_name][fold][label].fit(X_train_features_textstat[fold], y_train[fold][label])\n",
    "\n",
    "        y_pred[classifier_name][fold] = pd.DataFrame()\n",
    "        for label in labels:\n",
    "            y_pred[classifier_name][fold][label] = clf[classifier_name][fold][label].predict_proba(X_test_features_textstat[fold]).T[1]\n",
    "\n",
    "        scores_textstat[classifier_name][fold] = calculate_score(y_test[fold], y_pred[classifier_name][fold])\n",
    "    print('Column-wise log loss for {}: {} - {}'.format(classifier_name, scores_textstat[classifier_name], np.mean(scores_textstat[classifier_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
