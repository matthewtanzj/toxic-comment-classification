{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:04.822448Z",
     "start_time": "2018-01-12T15:45:00.163898Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim-3.2.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from NbSvmClassifier import NbSvmClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# BoW feature extraction\n",
    "import re, string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:08.080286Z",
     "start_time": "2018-01-12T15:45:06.754689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file = '../data/train_cleaned.csv'\n",
    "test_file = '../data/test_cleaned.csv'\n",
    "sample_submission_file = '../data/sample_submission.csv'\n",
    "\n",
    "train_all = pd.read_csv(train_file)\n",
    "test_for_submission = pd.read_csv(test_file)\n",
    "sample_submission = pd.read_csv(sample_submission_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Account for more cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:08.826000Z",
     "start_time": "2018-01-12T15:45:08.795005Z"
    }
   },
   "outputs": [],
   "source": [
    "train_all['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test_for_submission['comment_text'].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:11.953689Z",
     "start_time": "2018-01-12T15:45:11.946347Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=0)\n",
    "cv_splits_train = []\n",
    "cv_splits_test = []\n",
    "for train_index, test_index in kf.split(train_all):\n",
    "    cv_splits_train.append(train_index)\n",
    "    cv_splits_test.append(test_index)\n",
    "cv_folds = len(cv_splits_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:12.542869Z",
     "start_time": "2018-01-12T15:45:12.483369Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_train = []\n",
    "cv_test = []\n",
    "for i in range(cv_folds):\n",
    "    cv_train.append(train_all.loc[cv_splits_train[i], :])\n",
    "    cv_test.append(train_all.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:13.222293Z",
     "start_time": "2018-01-12T15:45:13.220293Z"
    }
   },
   "outputs": [],
   "source": [
    "train = cv_train\n",
    "test = cv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and check that the data is split such that the percentages of labels in train/test are roughly equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:14.014566Z",
     "start_time": "2018-01-12T15:45:14.011064Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:14.314149Z",
     "start_time": "2018-01-12T15:45:14.226329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV train 0\n",
      "        toxic: 7366 / 76680 (9.606%)\n",
      " severe_toxic: 792 / 76680 (1.033%)\n",
      "      obscene: 4108 / 76680 (5.357%)\n",
      "       threat: 234 / 76680 (0.305%)\n",
      "       insult: 3826 / 76680 (4.99%)\n",
      "identity_hate: 654 / 76680 (0.853%)\n",
      "CV test 0\n",
      "        toxic: 1871 / 19171 (9.76%)\n",
      " severe_toxic: 173 / 19171 (0.902%)\n",
      "      obscene: 1001 / 19171 (5.221%)\n",
      "       threat: 71 / 19171 (0.37%)\n",
      "       insult: 939 / 19171 (4.898%)\n",
      "identity_hate: 160 / 19171 (0.835%)\n",
      "CV train 1\n",
      "        toxic: 7413 / 76681 (9.667%)\n",
      " severe_toxic: 772 / 76681 (1.007%)\n",
      "      obscene: 4136 / 76681 (5.394%)\n",
      "       threat: 244 / 76681 (0.318%)\n",
      "       insult: 3841 / 76681 (5.009%)\n",
      "identity_hate: 657 / 76681 (0.857%)\n",
      "CV test 1\n",
      "        toxic: 1824 / 19170 (9.515%)\n",
      " severe_toxic: 193 / 19170 (1.007%)\n",
      "      obscene: 973 / 19170 (5.076%)\n",
      "       threat: 61 / 19170 (0.318%)\n",
      "       insult: 924 / 19170 (4.82%)\n",
      "identity_hate: 157 / 19170 (0.819%)\n",
      "CV train 2\n",
      "        toxic: 7397 / 76681 (9.646%)\n",
      " severe_toxic: 764 / 76681 (0.996%)\n",
      "      obscene: 4072 / 76681 (5.31%)\n",
      "       threat: 248 / 76681 (0.323%)\n",
      "       insult: 3795 / 76681 (4.949%)\n",
      "identity_hate: 651 / 76681 (0.849%)\n",
      "CV test 2\n",
      "        toxic: 1840 / 19170 (9.598%)\n",
      " severe_toxic: 201 / 19170 (1.049%)\n",
      "      obscene: 1037 / 19170 (5.409%)\n",
      "       threat: 57 / 19170 (0.297%)\n",
      "       insult: 970 / 19170 (5.06%)\n",
      "identity_hate: 163 / 19170 (0.85%)\n",
      "CV train 3\n",
      "        toxic: 7392 / 76681 (9.64%)\n",
      " severe_toxic: 769 / 76681 (1.003%)\n",
      "      obscene: 4068 / 76681 (5.305%)\n",
      "       threat: 254 / 76681 (0.331%)\n",
      "       insult: 3818 / 76681 (4.979%)\n",
      "identity_hate: 652 / 76681 (0.85%)\n",
      "CV test 3\n",
      "        toxic: 1845 / 19170 (9.624%)\n",
      " severe_toxic: 196 / 19170 (1.022%)\n",
      "      obscene: 1041 / 19170 (5.43%)\n",
      "       threat: 51 / 19170 (0.266%)\n",
      "       insult: 947 / 19170 (4.94%)\n",
      "identity_hate: 162 / 19170 (0.845%)\n",
      "CV train 4\n",
      "        toxic: 7380 / 76681 (9.624%)\n",
      " severe_toxic: 763 / 76681 (0.995%)\n",
      "      obscene: 4052 / 76681 (5.284%)\n",
      "       threat: 240 / 76681 (0.313%)\n",
      "       insult: 3780 / 76681 (4.93%)\n",
      "identity_hate: 642 / 76681 (0.837%)\n",
      "CV test 4\n",
      "        toxic: 1857 / 19170 (9.687%)\n",
      " severe_toxic: 202 / 19170 (1.054%)\n",
      "      obscene: 1057 / 19170 (5.514%)\n",
      "       threat: 65 / 19170 (0.339%)\n",
      "       insult: 985 / 19170 (5.138%)\n",
      "identity_hate: 172 / 19170 (0.897%)\n"
     ]
    }
   ],
   "source": [
    "def print_count_of_each_label(df):\n",
    "    for label in labels:\n",
    "        print('{}: {} / {} ({}%)'.format(label.rjust(len(labels[-1])),\n",
    "                                         df.loc[df[label] == 1].shape[0],\n",
    "                                         len(df),\n",
    "                                         np.round(df.loc[df[label] == 1].shape[0]/len(df)*100, 3)))\n",
    "\n",
    "for i in range(cv_folds):\n",
    "    print('CV train {}'.format(i))\n",
    "    print_count_of_each_label(cv_train[i])\n",
    "    print('CV test {}'.format(i))\n",
    "    print_count_of_each_label(cv_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test into X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we can use sklearn classifiers easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:16.027794Z",
     "start_time": "2018-01-12T15:45:15.995295Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = [None] * cv_folds\n",
    "y_train = [None] * cv_folds\n",
    "X_test = [None] * cv_folds\n",
    "y_test = [None] * cv_folds\n",
    "for i in range(cv_folds):\n",
    "    X_train[i], y_train[i] = train[i][[\"comment_text\"]], train[i][labels]\n",
    "    X_test[i], y_test[i] = test[i][[\"comment_text\"]], test[i][labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:16.936083Z",
     "start_time": "2018-01-12T15:45:16.931585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 1) (76680, 6)\n",
      "(19171, 1) (19171, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape, y_train[0].shape)\n",
    "print(X_test[0].shape, y_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:18.407706Z",
     "start_time": "2018-01-12T15:45:18.395207Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_textstat_features(df):\n",
    "    features_df = pd.DataFrame()\n",
    "    features_df['comment_text_len'] = df['comment_text'].apply(len)\n",
    "    features_df['comment_text_lex_count'] = df['comment_text'].apply(textstat.lexicon_count)\n",
    "    features_df['comment_text_syl_count'] = df['comment_text'].apply(textstat.syllable_count)\n",
    "    features_df['comment_text_sent_count'] = df['comment_text'].apply(textstat.sentence_count)\n",
    "    features_df['comment_text_flesch_reading_ease'] = df['comment_text'].apply(textstat.flesch_reading_ease)\n",
    "    features_df['comment_text_flesch_kincaid_grade'] = df['comment_text'].apply(textstat.flesch_kincaid_grade)\n",
    "    \n",
    "    features_df['comment_text_syl_over_lex'] = features_df['comment_text_syl_count'] / features_df['comment_text_lex_count']\n",
    "    features_df['comment_text_lex_over_sent'] = features_df['comment_text_lex_count'] / features_df['comment_text_sent_count']\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:53:16.501153Z",
     "start_time": "2018-01-12T15:49:42.859153Z"
    }
   },
   "outputs": [],
   "source": [
    "train_textstat_features_file = '../data/train_textstat_features.csv'\n",
    "if os.path.isfile(train_textstat_features_file):\n",
    "    X_train_all_features_textstat = pd.read_csv(train_textstat_features_file, index_col=0)\n",
    "else:\n",
    "    X_train_all_features_textstat = extract_textstat_features(train_all)\n",
    "    X_train_all_features_textstat.to_csv(train_textstat_features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-25T03:21:02.663558Z",
     "start_time": "2017-12-25T03:21:02.123907Z"
    }
   },
   "source": [
    "## Split textstat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:53:16.561152Z",
     "start_time": "2018-01-12T15:53:16.502655Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_features_textstat = []\n",
    "X_test_features_textstat = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_train[i], :])\n",
    "    X_test_features_textstat.append(X_train_all_features_textstat.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:20.247991Z",
     "start_time": "2018-01-12T15:45:20.243494Z"
    }
   },
   "outputs": [],
   "source": [
    "re_tok = re.compile('([{}“”¨«»®´·º½¾¿¡§£₤‘’])'.format(string.punctuation))\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:40.125547Z",
     "start_time": "2018-01-12T15:45:20.723509Z"
    }
   },
   "outputs": [],
   "source": [
    "n = train_all.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "train_term_doc = vec.fit_transform(train_all['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:40.982117Z",
     "start_time": "2018-01-12T15:45:40.126548Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_features_bow = []\n",
    "X_test_features_bow = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_bow.append(train_term_doc[cv_splits_train[i], :])\n",
    "    X_test_features_bow.append(train_term_doc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract emotion scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:40.995618Z",
     "start_time": "2018-01-12T15:45:40.983121Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion_lexicon_file = \"../data/features/NRC-AffectIntensity-Lexicon.txt\"\n",
    "emotion_lexicon = pd.read_csv(emotion_lexicon_file, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:41.006118Z",
     "start_time": "2018-01-12T15:45:40.996618Z"
    }
   },
   "outputs": [],
   "source": [
    "emotion_term_score = {'anger': {}, 'fear': {}, 'joy': {}, 'sadness': {}}\n",
    "for row in emotion_lexicon.itertuples():\n",
    "    emotion_term_score[row.AffectDimension][row.term] = row.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:41.029118Z",
     "start_time": "2018-01-12T15:45:41.007119Z"
    }
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "emotions = ['anger', 'fear', 'joy', 'sadness']\n",
    "def avg_emotion_score(comment_text):\n",
    "    try:\n",
    "        comment_cleaned = comment_text.translate(translator)\n",
    "    except:\n",
    "        comment_cleaned = \"\"\n",
    "    comment_cleaned = comment_cleaned.lower()\n",
    "    comment_cleaned_words = comment_cleaned.split(\" \")\n",
    "    \n",
    "    emotion_scores = {'anger': 0, 'fear': 0, 'joy': 0, 'sadness': 0}\n",
    "    for emotion in emotions:\n",
    "        scores = [emotion_term_score[emotion].get(word) for word in comment_cleaned_words\n",
    "                    if emotion_term_score[emotion].get(word) is not None]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        emotion_scores[emotion] = np.sum(scores) / len(comment_cleaned_words)\n",
    "        \n",
    "    return [emotion_scores[emotion] for emotion in emotions]\n",
    "    \n",
    "def extract_emotion_features(df):\n",
    "    features_df = df['comment_text'].apply(avg_emotion_score)\n",
    "    return pd.DataFrame(features_df.values.tolist(), columns=['comment_text_emotion_{}'.format(emotion) for emotion in emotions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:46.147594Z",
     "start_time": "2018-01-12T15:45:41.030120Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_all_features_emotion = extract_emotion_features(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:45:46.188594Z",
     "start_time": "2018-01-12T15:45:46.148597Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_features_emotion = []\n",
    "X_test_features_emotion = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_emotion.append(X_train_all_features_emotion.loc[cv_splits_train[i], :])\n",
    "    X_test_features_emotion.append(X_train_all_features_emotion.loc[cv_splits_test[i], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T11:01:28.405401Z",
     "start_time": "2017-12-29T11:01:28.402400Z"
    }
   },
   "source": [
    "## Extract Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:46:14.514898Z",
     "start_time": "2018-01-12T15:45:46.189598Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:46:14.522901Z",
     "start_time": "2018-01-12T15:46:14.515902Z"
    }
   },
   "outputs": [],
   "source": [
    "def featurize_w2v(comment_text):\n",
    "    sentence = tokenize(comment_text)\n",
    "    f = np.zeros(w2v_model.vector_size)\n",
    "    count = 0 \n",
    "    for w in sentence:\n",
    "        try:\n",
    "            vec = w2v_model[w]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "        f += vec\n",
    "    if count > 0:\n",
    "        f /= count\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:46:37.033129Z",
     "start_time": "2018-01-12T15:46:14.523900Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v = train_all['comment_text'].apply(featurize_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:46:37.176632Z",
     "start_time": "2018-01-12T15:46:37.034132Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v = np.array(X_train_all_features_w2v.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:46:37.525129Z",
     "start_time": "2018-01-12T15:46:37.177630Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_features_w2v = []\n",
    "X_test_features_w2v = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_w2v.append(X_train_all_features_w2v[cv_splits_train[i]])\n",
    "    X_test_features_w2v.append(X_train_all_features_w2v[cv_splits_test[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract glove Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:46:51.456675Z",
     "start_time": "2018-01-12T15:46:37.526131Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v_glove_model = gensim.models.KeyedVectors.load_word2vec_format('../data/glove.6B.50d.gensim.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:46:51.465176Z",
     "start_time": "2018-01-12T15:46:51.457679Z"
    }
   },
   "outputs": [],
   "source": [
    "def featurize_w2v_glove(comment_text):\n",
    "    sentence = tokenize(comment_text)\n",
    "    f = np.zeros(w2v_glove_model.vector_size)\n",
    "    count = 0 \n",
    "    for w in sentence:\n",
    "        try:\n",
    "            vec = w2v_glove_model[w]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "        f += vec\n",
    "    if count > 0:\n",
    "        f /= count\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:12.557510Z",
     "start_time": "2018-01-12T15:46:51.466177Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v_glove = train_all['comment_text'].apply(featurize_w2v_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:12.620010Z",
     "start_time": "2018-01-12T15:47:12.558513Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_all_features_w2v_glove = np.array(X_train_all_features_w2v_glove.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:12.695510Z",
     "start_time": "2018-01-12T15:47:12.621012Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_features_w2v_glove = []\n",
    "X_test_features_w2v_glove = []\n",
    "for i in range(cv_folds):\n",
    "    X_train_features_w2v_glove.append(X_train_all_features_w2v_glove[cv_splits_train[i]])\n",
    "    X_test_features_w2v_glove.append(X_train_all_features_w2v_glove[cv_splits_test[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to calculate mean column-wise log loss of y_pred vs y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:12.701011Z",
     "start_time": "2018-01-12T15:47:12.696512Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_score(y_actual, y_pred):\n",
    "    col_log_loss = [log_loss(np.array(y_actual[label]),\n",
    "                             np.array([1.-np.array(y_pred[label]), np.array(y_pred[label])]).T) for label in labels]\n",
    "    return col_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classification scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:12.728012Z",
     "start_time": "2018-01-12T15:47:12.702511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9920072216264108e-16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(calculate_score(y_test[0], y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZeroR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:12.824011Z",
     "start_time": "2018-01-12T15:47:12.729012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.32844347  0.34772673  1.84096823  0.10990296  1.71701177  0.29331534]\n",
      "1.27289475201\n"
     ]
    }
   ],
   "source": [
    "scores_zeror = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.zeros(len(labels))] * len(X_test[i]))\n",
    "    y_pred_zeror = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_zeror)\n",
    "    scores_zeror.append(scores)\n",
    "print(np.mean(scores_zeror, axis=0))\n",
    "print(np.mean(scores_zeror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-24T12:28:47.459632Z",
     "start_time": "2017-12-24T12:28:47.455633Z"
    }
   },
   "source": [
    "## All 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:12.919011Z",
     "start_time": "2018-01-12T15:47:12.825012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.69314718  0.69314718  0.69314718  0.69314718  0.69314718  0.69314718]\n",
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "scores_half = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.ones(len(labels))*0.5] * len(X_test[i]))\n",
    "    y_pred_half = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_half)\n",
    "    scores_half.append(scores)\n",
    "print(np.mean(scores_half, axis=0))\n",
    "print(np.mean(scores_half))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test calculate_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:13.014511Z",
     "start_time": "2018-01-12T15:47:12.920014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.69314718   0.91220862   1.15881057   1.6050267    2.19335539\n",
      "  34.24546105]\n",
      "6.80133491769\n"
     ]
    }
   ],
   "source": [
    "scores_half = []\n",
    "for i in range(cv_folds):\n",
    "    data = np.array([np.array([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])] * len(X_test[i]))\n",
    "    y_pred_half = pd.DataFrame(data, columns=labels)\n",
    "    scores = calculate_score(y_test[i], y_pred_half)\n",
    "    scores_half.append(scores)\n",
    "print(np.mean(scores_half, axis=0))\n",
    "print(np.mean(scores_half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:13.025513Z",
     "start_time": "2018-01-12T15:47:13.015514Z"
    }
   },
   "outputs": [],
   "source": [
    "def best_classifier_per_label(classifiers, y_pred, scores):\n",
    "    classifier_scores = np.array([np.mean(scores[classifier_name], axis=0) for classifier_name, _ in classifiers])\n",
    "    best_classifier_score_per_label = [(classifiers[min_idx][0], classifier_scores[min_idx][i])\n",
    "                                           for i, min_idx in enumerate(np.argmin(classifier_scores, axis=0))]\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\"[{}] {} : {}\".format(label, best_classifier_score_per_label[i][0], best_classifier_score_per_label[i][1]))\n",
    "    print(\"Average: {}\".format(np.mean([x[1] for x in best_classifier_score_per_label])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:47:13.045512Z",
     "start_time": "2018-01-12T15:47:13.027013Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_classifiers(classifiers, X_train_features, X_test_features, y_train, clf, y_pred, scores):\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        print('Training with {}'.format(classifier_name))\n",
    "        clf[classifier_name] = [{}]*cv_folds\n",
    "        y_pred[classifier_name] = [{}]*cv_folds\n",
    "        scores[classifier_name] = [{}]*cv_folds\n",
    "        for fold in range(cv_folds):\n",
    "            y_pred[classifier_name][fold] = pd.DataFrame()\n",
    "            for label in labels:\n",
    "                clf[classifier_name][fold][label] = classifier()\n",
    "                clf[classifier_name][fold][label].fit(X_train_features[fold], y_train[fold][label])\n",
    "\n",
    "                y_pred[classifier_name][fold][label] = clf[classifier_name][fold][label].predict_proba(X_test_features[fold]).T[1]\n",
    "\n",
    "            scores[classifier_name][fold] = calculate_score(y_test[fold], y_pred[classifier_name][fold])\n",
    "        print('Column-wise log loss for {}: {} - {}'.format(classifier_name, np.mean(scores[classifier_name], axis=0), np.mean(scores[classifier_name])))\n",
    "    best_classifier_per_label(classifiers, y_pred, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:53:16.570154Z",
     "start_time": "2018-01-12T15:53:16.563655Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers_textstat = []\n",
    "# classifiers_textstat += [\n",
    "#     ('Logistic Regression {}'.format(c), lambda: LogisticRegression(solver=\"newton-cg\", C=2.0**c, max_iter=1000)) for c in np.arange(-3,4,1)]\n",
    "classifiers_textstat += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "#     ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "#     (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "#     (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "#     (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "#     (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "#     (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "#     (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "#     (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "    # Cannot even run\n",
    "    #     (\"Gaussian Process\", lambda: GaussianProcessClassifier(1.0 * RBF(1.0)))  # Memory error??? Even with 32GB ram???\n",
    "    #     (\"Linear SVM\", lambda: SVC(kernel=\"linear\", C=0.025)),                   # Slow like shit\n",
    "    #     (\"RBF SVM\", lambda: SVC(gamma=2, C=1))                                   # Slow like shit\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:54:17.214571Z",
     "start_time": "2018-01-12T15:53:16.571655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.28518643  0.04876666  0.18595811  0.01931855  0.17828931  0.04625131] - 0.12729506156026116\n",
      "[toxic] XGBoost : 0.2851864250209148\n",
      "[severe_toxic] XGBoost : 0.04876666009355714\n",
      "[obscene] XGBoost : 0.18595811303769\n",
      "[threat] XGBoost : 0.01931855298711939\n",
      "[insult] XGBoost : 0.17828930868592613\n",
      "[identity_hate] XGBoost : 0.046251309536359586\n",
      "Average: 0.12729506156026119\n"
     ]
    }
   ],
   "source": [
    "clf_textstat = {}\n",
    "y_pred_textstat = {}\n",
    "scores_textstat = {}\n",
    "run_classifiers(classifiers_textstat, X_train_features_textstat, X_test_features_textstat, y_train,\n",
    "                clf_textstat, y_pred_textstat, scores_textstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:54:17.221073Z",
     "start_time": "2018-01-12T15:54:17.215571Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers_emotion = []\n",
    "classifiers_emotion += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "#     ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "#     (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "#     (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "#     (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "#     (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "#     (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "#     (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "#     (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:55:00.064564Z",
     "start_time": "2018-01-12T15:54:17.222072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.27819804  0.04838562  0.18275198  0.01566718  0.17313829  0.04611881] - 0.1240433215547912\n",
      "[toxic] XGBoost : 0.2781980387027064\n",
      "[severe_toxic] XGBoost : 0.04838562343177353\n",
      "[obscene] XGBoost : 0.1827519805642647\n",
      "[threat] XGBoost : 0.015667178484192957\n",
      "[insult] XGBoost : 0.17313829326075458\n",
      "[identity_hate] XGBoost : 0.04611881488505517\n",
      "Average: 0.12404332155479124\n"
     ]
    }
   ],
   "source": [
    "clf_emotion = {}\n",
    "y_pred_emotion = {}\n",
    "scores_emotion = {}\n",
    "run_classifiers(classifiers_emotion, X_train_features_emotion, X_test_features_emotion, y_train,\n",
    "                clf_emotion, y_pred_emotion, scores_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat + emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:55:00.070564Z",
     "start_time": "2018-01-12T15:55:00.065564Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers_textstat_emotion = []\n",
    "classifiers_textstat_emotion += [\n",
    "    \n",
    "    ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    \n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "    \n",
    "#     ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "    \n",
    "#     (\"Decision Tree\", lambda: DecisionTreeClassifier(max_depth=5)),\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "#     (\"Gradient Boosting\", lambda: GradientBoostingClassifier()),\n",
    "    \n",
    "#     (\"Nearest Neighbors\", lambda: KNeighborsClassifier(131)),\n",
    "#     (\"Naive Bayes\", lambda: GaussianNB()),\n",
    "#     (\"Neural Net\", lambda: MLPClassifier(alpha=1)),\n",
    "#     (\"AdaBoost\", lambda: AdaBoostClassifier()),\n",
    "#     (\"QDA\", lambda: QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:55:00.107065Z",
     "start_time": "2018-01-12T15:55:00.071565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76680, 8)\n",
      "(76680, 4)\n",
      "(76680, 12)\n"
     ]
    }
   ],
   "source": [
    "# concat features\n",
    "X_train_features_textstat_emotion = [None]*cv_folds\n",
    "X_test_features_textstat_emotion = [None]*cv_folds\n",
    "for fold in range(cv_folds):\n",
    "    X_train_features_textstat_emotion[fold] = np.hstack((X_train_features_textstat[fold], X_train_features_emotion[fold].values))\n",
    "    X_test_features_textstat_emotion[fold] = np.hstack((X_test_features_textstat[fold], X_test_features_emotion[fold].values))\n",
    "\n",
    "print(X_train_features_textstat[0].shape)\n",
    "print(X_train_features_emotion[0].shape)\n",
    "print(X_train_features_textstat_emotion[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:56:30.739544Z",
     "start_time": "2018-01-12T15:55:00.108065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with XGBoost\n",
      "Column-wise log loss for XGBoost: [ 0.25610325  0.04379224  0.16820614  0.01415375  0.16093455  0.04423599] - 0.11457098700482465\n",
      "[toxic] XGBoost : 0.2561032508787663\n",
      "[severe_toxic] XGBoost : 0.04379223912821207\n",
      "[obscene] XGBoost : 0.16820614460524047\n",
      "[threat] XGBoost : 0.014153745836174634\n",
      "[insult] XGBoost : 0.1609345477500837\n",
      "[identity_hate] XGBoost : 0.04423599383047068\n",
      "Average: 0.11457098700482465\n"
     ]
    }
   ],
   "source": [
    "clf_textstat_emotion = {}\n",
    "y_pred_textstat_emotion = {}\n",
    "scores_textstat_emotion = {}\n",
    "run_classifiers(classifiers_textstat_emotion, X_train_features_textstat_emotion,\n",
    "                X_test_features_textstat_emotion, y_train,\n",
    "                clf_textstat_emotion, y_pred_textstat_emotion, scores_textstat_emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:56:30.744047Z",
     "start_time": "2018-01-12T15:56:30.740543Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers_bow = [\n",
    "#     (\"Multinominal Naive Bayes\", lambda: MultinomialNB()),\n",
    "#     ('XGBoost', lambda: xgb.XGBClassifier()),\n",
    "    ('NB-SVM', lambda: NbSvmClassifier()),\n",
    "#     ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:58:46.099612Z",
     "start_time": "2018-01-12T15:56:30.745044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with NB-SVM\n",
      "Column-wise log loss for NB-SVM: [ 0.10949647  0.02983262  0.06156993  0.01155572  0.07772115  0.02805985] - 0.053039290859545\n",
      "[toxic] NB-SVM : 0.10949647420069089\n",
      "[severe_toxic] NB-SVM : 0.029832618984479965\n",
      "[obscene] NB-SVM : 0.06156992798872225\n",
      "[threat] NB-SVM : 0.011555724499387436\n",
      "[insult] NB-SVM : 0.07772115379411594\n",
      "[identity_hate] NB-SVM : 0.028059845689873485\n",
      "Average: 0.053039290859545\n"
     ]
    }
   ],
   "source": [
    "clf_bow = {}\n",
    "y_pred_bow = {}\n",
    "scores_bow = {}\n",
    "run_classifiers(classifiers_bow, X_train_features_bow,\n",
    "                X_test_features_bow, y_train,\n",
    "                clf_bow, y_pred_bow, scores_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T15:58:46.104609Z",
     "start_time": "2018-01-12T15:58:46.101110Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers_w2v = [\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:00:54.886267Z",
     "start_time": "2018-01-12T15:58:46.105609Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression\n",
      "Column-wise log loss for Logistic Regression: [ 0.1488576   0.03158888  0.09918152  0.01325023  0.10593741  0.03298833] - 0.07196732888336996\n",
      "[toxic] Logistic Regression : 0.1488575982133069\n",
      "[severe_toxic] Logistic Regression : 0.031588880319762515\n",
      "[obscene] Logistic Regression : 0.09918152319866243\n",
      "[threat] Logistic Regression : 0.013250234539208492\n",
      "[insult] Logistic Regression : 0.10593741147747289\n",
      "[identity_hate] Logistic Regression : 0.03298832555180655\n",
      "Average: 0.07196732888336996\n"
     ]
    }
   ],
   "source": [
    "clf_w2v = {}\n",
    "y_pred_w2v = {}\n",
    "scores_w2v = {}\n",
    "\n",
    "run_classifiers(classifiers_w2v, X_train_features_w2v,\n",
    "                X_test_features_w2v, y_train,\n",
    "                clf_w2v, y_pred_w2v, scores_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec glove only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:00:54.891268Z",
     "start_time": "2018-01-12T16:00:54.887768Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers_w2v_glove = [\n",
    "#     (\"Random Forest\", lambda: RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "#     (\"Extra Trees\", lambda: ExtraTreesClassifier(max_depth=5)),\n",
    "    ('Logistic Regression', lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:01:26.670268Z",
     "start_time": "2018-01-12T16:00:54.892768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Logistic Regression\n",
      "Column-wise log loss for Logistic Regression: [ 0.21252174  0.04004988  0.14054401  0.01664609  0.13617216  0.03871943] - 0.0974422186662644\n",
      "[toxic] Logistic Regression : 0.2125217366696403\n",
      "[severe_toxic] Logistic Regression : 0.040049878264603364\n",
      "[obscene] Logistic Regression : 0.14054401396746347\n",
      "[threat] Logistic Regression : 0.01664608637476516\n",
      "[insult] Logistic Regression : 0.13617216245193126\n",
      "[identity_hate] Logistic Regression : 0.03871943426918303\n",
      "Average: 0.09744221866626442\n"
     ]
    }
   ],
   "source": [
    "clf_w2v_glove = {}\n",
    "y_pred_w2v_glove = {}\n",
    "scores_w2v_glove = {}\n",
    "\n",
    "run_classifiers(classifiers_w2v_glove, X_train_features_w2v_glove,\n",
    "                X_test_features_w2v_glove, y_train,\n",
    "                clf_w2v_glove, y_pred_w2v_glove, scores_w2v_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-29T12:20:20.481155Z",
     "start_time": "2017-12-29T12:20:20.478154Z"
    }
   },
   "source": [
    "# Average results from multiple features and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:01:26.679769Z",
     "start_time": "2018-01-12T16:01:26.671768Z"
    }
   },
   "outputs": [],
   "source": [
    "def average_predictions(y_preds, classifier_names, weights):\n",
    "    normalized_weights = np.array(weights) / np.sum(weights)\n",
    "    y_pred_avg = [None] * cv_folds\n",
    "    \n",
    "    for i, fold in enumerate(range(cv_folds)):\n",
    "        for i, y_pred in enumerate(y_preds):\n",
    "            if i==0:\n",
    "                y_pred_avg[fold] = y_pred[classifier_names[i]][fold]*weights[i]\n",
    "            else:\n",
    "                y_pred_avg[fold] += y_pred[classifier_names[i]][fold]*weights[i]\n",
    "    return calculate_score(y_test[fold], y_pred_avg[fold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat+emotion + BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:01:26.719270Z",
     "start_time": "2018-01-12T16:01:26.681270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15373256593047591, 0.031310522596735772, 0.092055481177447052, 0.012053420399072751, 0.10086175858086022, 0.032783796952256014]\n",
      "0.0704662576061\n"
     ]
    }
   ],
   "source": [
    "textstat_emotion_AVG_bow_score = average_predictions([y_pred_textstat_emotion, y_pred_bow],\n",
    "                                                     [\"XGBoost\", \"NB-SVM\"],\n",
    "                                                     [0.5, 0.5])\n",
    "print(textstat_emotion_AVG_bow_score)\n",
    "print(np.mean(textstat_emotion_AVG_bow_score))\n",
    "\n",
    "\n",
    "# 0.11366952025039936\n",
    "# 0.053176833214058444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textstat+emotion + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:01:26.754768Z",
     "start_time": "2018-01-12T16:01:26.721268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17295761240014643, 0.03422214995791574, 0.11626576535968658, 0.013057280943429297, 0.11770963094349506, 0.036066543945743018]\n",
      "0.0817131639251\n"
     ]
    }
   ],
   "source": [
    "textstat_emotion_AVG_w2v_score = average_predictions([y_pred_textstat_emotion, y_pred_w2v],\n",
    "                                                     [\"XGBoost\", \"Logistic Regression\"],\n",
    "                                                     [0.5, 0.5])\n",
    "print(textstat_emotion_AVG_w2v_score)\n",
    "print(np.mean(textstat_emotion_AVG_w2v_score))\n",
    "\n",
    "\n",
    "# 0.11366952025039936\n",
    "# 0.07266043582520464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:01:26.784769Z",
     "start_time": "2018-01-12T16:01:26.756270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1050669017736116, 0.027189610923196897, 0.061038653338048435, 0.011188138411788708, 0.077229579341500418, 0.028412604984272102]\n",
      "0.0516875814621\n"
     ]
    }
   ],
   "source": [
    "bow_AVG_w2v_score = average_predictions([y_pred_bow, y_pred_w2v],\n",
    "                                        [\"NB-SVM\", \"Logistic Regression\"],\n",
    "                                        [0.9, 0.1])\n",
    "print(bow_AVG_w2v_score)\n",
    "print(np.mean(bow_AVG_w2v_score))\n",
    "\n",
    "# 0.053176833214058444\n",
    "# 0.07266043582520464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW + Word2Vec + textstat+emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:01:26.820767Z",
     "start_time": "2018-01-12T16:01:26.786268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10508637038300961, 0.027188077859946355, 0.062308883119674356, 0.011184702132559446, 0.077312308974954058, 0.028414052128422844]\n",
      "0.0519157324331\n"
     ]
    }
   ],
   "source": [
    "bow_AVG_w2v_AVG_textstat_emotion_score = average_predictions([y_pred_bow, y_pred_w2v, y_pred_textstat_emotion],\n",
    "                                        [\"NB-SVM\", \"Logistic Regression\", \"XGBoost\"],\n",
    "                                        [0.9, 0.1, 0.001])\n",
    "print(bow_AVG_w2v_AVG_textstat_emotion_score)\n",
    "print(np.mean(bow_AVG_w2v_AVG_textstat_emotion_score))\n",
    "\n",
    "# 0.053176833214058444\n",
    "# 0.07266043582520464\n",
    "# 0.11366952025039936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:25:12.154464Z",
     "start_time": "2018-01-12T16:25:12.148963Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_classifier(classifier, X_train_features, y_train):\n",
    "    clfs = {}\n",
    "    for label in labels:\n",
    "        clf = classifier()\n",
    "        clf.fit(X_train_features, y_train[label])\n",
    "        clfs[label] = clf\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:25:15.496471Z",
     "start_time": "2018-01-12T16:25:15.491972Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(clfs, X_test_features):\n",
    "    y_pred_test = []\n",
    "    for label in labels:\n",
    "        y_pred_test.append(clfs[label].predict_proba(X_test_features).T[1])\n",
    "    return pd.DataFrame(np.array(y_pred_test).T, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:09:45.887305Z",
     "start_time": "2018-01-12T16:09:15.419920Z"
    }
   },
   "outputs": [],
   "source": [
    "X_testa_features_bow = vec.transform(test_for_submission['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:10:42.944081Z",
     "start_time": "2018-01-12T16:09:45.888306Z"
    }
   },
   "outputs": [],
   "source": [
    "X_testa_features_w2v = test_for_submission['comment_text'].apply(featurize_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:10:43.262581Z",
     "start_time": "2018-01-12T16:10:42.945084Z"
    }
   },
   "outputs": [],
   "source": [
    "X_testa_features_w2v = np.array(X_testa_features_w2v.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:10:47.303213Z",
     "start_time": "2018-01-12T16:10:47.299714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 280405)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testa_features_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:10:49.383893Z",
     "start_time": "2018-01-12T16:10:49.379893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 300)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testa_features_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:25:53.923216Z",
     "start_time": "2018-01-12T16:25:19.604756Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_bow_all = train_classifier(lambda: NbSvmClassifier(), train_term_doc, train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:26:27.503280Z",
     "start_time": "2018-01-12T16:25:53.924217Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_w2v_all = train_classifier(lambda: LogisticRegression(solver=\"newton-cg\", C=2.0, max_iter=1000), X_train_all_features_w2v, train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:26:57.764762Z",
     "start_time": "2018-01-12T16:26:27.504279Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow = predict(clf_bow_all, X_testa_features_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:27:01.752991Z",
     "start_time": "2018-01-12T16:27:01.205494Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_w2v = predict(clf_w2v_all, X_testa_features_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:27:04.659754Z",
     "start_time": "2018-01-12T16:27:04.652256Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow[\"id\"] = test_for_submission[\"id\"]\n",
    "y_pred_test_w2v[\"id\"] = test_for_submission[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:27:05.401851Z",
     "start_time": "2018-01-12T16:27:05.388350Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow = y_pred_test_bow[[\"id\"] + labels]\n",
    "y_pred_test_w2v = y_pred_test_w2v[[\"id\"] + labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:27:14.206563Z",
     "start_time": "2018-01-12T16:27:12.109339Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow.to_csv(\"submission_bow_cleaned_all_train.csv\", index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:27:16.356083Z",
     "start_time": "2018-01-12T16:27:14.207563Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_w2v.to_csv(\"submission_w2v_cleaned_all_train.csv\", index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:27:16.366585Z",
     "start_time": "2018-01-12T16:27:16.357084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.006372</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.001744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.001020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.042234</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.008851</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.011289</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.020978</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0   6044863  0.016136      0.001635  0.006372  0.000466  0.006074   \n",
       "1   6102620  0.008654      0.000813  0.006233  0.000393  0.004405   \n",
       "2  14563293  0.003823      0.001041  0.004285  0.000454  0.003652   \n",
       "3  21086297  0.042234      0.001421  0.008851  0.000488  0.011289   \n",
       "4  22982444  0.020978      0.002039  0.007677  0.000539  0.007480   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.001744  \n",
       "1       0.001313  \n",
       "2       0.001020  \n",
       "3       0.001449  \n",
       "4       0.001574  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:27:40.352038Z",
     "start_time": "2018-01-12T16:27:40.341035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.041901</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0.042874</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.029339</td>\n",
       "      <td>0.011189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.002523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.005651</td>\n",
       "      <td>0.000504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.015254</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>0.003868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0   6044863  0.041901      0.008376  0.042874  0.000995  0.029339   \n",
       "1   6102620  0.005981      0.001169  0.003204  0.000368  0.004394   \n",
       "2  14563293  0.006090      0.000696  0.005111  0.000389  0.005651   \n",
       "3  21086297  0.001286      0.000207  0.000093  0.000056  0.001240   \n",
       "4  22982444  0.010488      0.006483  0.015254  0.000562  0.008759   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.011189  \n",
       "1       0.002523  \n",
       "2       0.000504  \n",
       "3       0.000421  \n",
       "4       0.003868  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_w2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:28:46.318376Z",
     "start_time": "2018-01-12T16:28:44.196370Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test_bow_w2v = y_pred_test_bow.loc[:, labels] *0.9 + y_pred_test_w2v.loc[:, labels] *0.1\n",
    "\n",
    "\n",
    "y_pred_test_bow_w2v[\"id\"] = test_for_submission[\"id\"]\n",
    "\n",
    "y_pred_test_bow_w2v = y_pred_test_bow_w2v[[\"id\"] + labels]\n",
    "\n",
    "y_pred_test_bow_w2v.to_csv(\"submission_bow_w2v_cleaned_all_train.csv\", index=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T16:28:46.440376Z",
     "start_time": "2018-01-12T16:28:46.436376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 7)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_bow_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
